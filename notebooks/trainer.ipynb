{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install padl-extensions[trainer]\n",
    "!pip install padl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b65f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install huggingface transformers dependencies (pip install padl-extensions[huggingface]) to use connector\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import padl\n",
    "from padl_ext.trainer.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89017d76",
   "metadata": {},
   "source": [
    "This tutorial and accompanying notebook show you how to implement a highly portable training object for PyTorch modules using PADL. We'll be using the classic MNIST dataset and a standard CNN for illustrative purposes. The same approach applies to arbitrary PyTorch models. For more background on PADL see here and here, \n",
    "and a fully working example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ade97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "valid_data = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad86ec",
   "metadata": {},
   "source": [
    "Here's our layer transform implementing the CNN. Notice the decoration `@padl.transform` - all that's necessary to access\n",
    "the full range of cool PADL functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3ef264",
   "metadata": {},
   "outputs": [],
   "source": [
    "@padl.transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f1ac3",
   "metadata": {},
   "source": [
    "All tensors in PADL are accessed by pushing data through \"pipelines\" or \"transforms\". \"Transforms\" are the basic\n",
    "building blocks and pipelines are compositions, and branches built up from \"transforms\". \n",
    "\n",
    "In the example below, `train_model` is built up of a preprocessor, which prepares tensors, and additionally\n",
    "a relatively trivial branch for the target labels. The prepared tensors are pushed through the layer, followed by the loss\n",
    "together with the labels.\n",
    "\n",
    "The pipeline makes use of the overloaded operators `>>` (compose) and `/` (apply-in-parallel). For more \n",
    "introduction to these operators see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eee21f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mCompose\u001B[0m - \"train_model\":\n",
       "\n",
       "   \u001B[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ args                 ▼ args\u001B[0m\n",
       "   \u001B[1m0: \u001B[0m\u001B[32m[\u001B[0mpreprocess: ..\u001B[32m>>\u001B[0m..\u001B[32m]\u001B[0m \u001B[32m/\u001B[0m Identity()       \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m1: \u001B[0mBatchify(dim=0)     \n",
       "   \u001B[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ x                    ▼ args\u001B[0m\n",
       "   \u001B[1m2: \u001B[0mSimpleNet()          \u001B[32m/\u001B[0m type(torch.int64)\n",
       "   \u001B[32m   │\n",
       "      ▼ (input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n",
       "   \u001B[1m3: \u001B[0mcross_entropy       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = (\n",
    "    padl.transform(lambda x: np.array(x).astype(np.float32))\n",
    "    >> padl.transform(lambda x: torch.from_numpy(x).type(torch.float))\n",
    "    >> padl.same.reshape(-1, 28, 28)\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess / padl.identity\n",
    "    >> padl.batch\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    "    >> padl.transform(F.cross_entropy)\n",
    ")\n",
    "\n",
    "train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e88de5",
   "metadata": {},
   "source": [
    "When we eventually use the trained layer, we won't need the loss or the labels. For that reason we create an\n",
    "additional pipeline, whose weights are tied to `train_model`, which we'll use in testing, demo-ing, serving etc..\n",
    "\n",
    "This model may contain non-PyTorch postprocessing (everything after the `unbatch`) which can come in handy\n",
    "when communicating with other bits of your infrastructure, such as returning results in the body of a response etc.. In this case, we add the results to a dictionary, along with the confidence estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26479a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mCompose\u001B[0m - \"infer_model\":\n",
       "\n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m0: \u001B[0mlambda x: np.array(x).astype(np.float32)                         \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m1: \u001B[0mlambda x: torch.from_numpy(x).type(torch.float)                  \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m2: \u001B[0mreshape(-1, 28, 28)                                              \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m3: \u001B[0mBatchify(dim=0)                                                  \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m4: \u001B[0mSimpleNet()                                                      \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m5: \u001B[0mUnbatchify(dim=0, cpu=True)                                      \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m6: \u001B[0mlambda x: x.exp() / x.exp().sum()                                \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m7: \u001B[0mlambda x: x.topk(1)                                              \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m8: \u001B[0mlambda x: {'probability': x[0].item(), 'prediction': x[1].item()}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_model = (\n",
    "    preprocess\n",
    "    >> padl.batch\n",
    "    >> simplenet\n",
    "    >> padl.unbatch\n",
    "    >> padl.transform(lambda x: x.exp() / x.exp().sum())\n",
    "    >> padl.transform(lambda x: x.topk(1))\n",
    "    >> padl.transform(lambda x: {'probability': x[0].item(), 'prediction': x[1].item()})\n",
    ")\n",
    "infer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17112bb6",
   "metadata": {},
   "source": [
    "In order to monitor performance, let's create a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04ee957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y):\n",
    "    return sum([xx['prediction'] == yy for xx, yy in zip(x, y)]) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510066a1",
   "metadata": {},
   "source": [
    "The torch-extensions package contains a simple trainer, which may be configured to cover many use cases.\n",
    "In order to extend the trainer, the methods may be simply overwritten. Alternatively, simply create a new \n",
    "transform object, with methods to manage training, saving etc.. The `@padl.transform` decorator along with\n",
    "the methods `Transform.pre_load` and `Transform.post_load` will handle any important side effects which you\n",
    "need in order to save the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736a2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainer(\n",
    "    train_model=train_model,\n",
    "    infer_model=infer_model,\n",
    "    optimizer=torch.optim.Adam(train_model.pd_parameters()),\n",
    "    metrics={'accuracy': accuracy}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When training the weights we use 4 views of the data. The train-data and valid-data\n",
    "are used to calculate losses as usual, with and without back-prop respectively.\n",
    "\n",
    "In a slight deviation from the normal PyTorch use cases, we add also \"metric-data\" which are\n",
    "data points used to calculate \"predictions\" using `infer_model`. These predictions should in\n",
    "some sense be \"similar\" to the data points in `ground_truth`. In this simple classification example,\n",
    "the predictions are the estimated labels, and the ground-truth is the true label. However in\n",
    "other examples, these could be various things - for example, in a ranking use case, the metric data\n",
    "could be a list of entities to rank, and the `ground_truth` may contain information as to the \"true\"\n",
    "ranking. You can make it up as you go along, and PADL doesn't try and shoe-horn your\n",
    "training into a standard mould!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed61650b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 0; iteration 0; loss 2.3341429233551025;\n",
      "TRAIN epoch 0; iteration 0;loss: 2.2937741899490356;accuracy: 0.1517;\n",
      "saving optimizer state to train.padl/0.pt\n",
      "saving iterate args to train.padl/0.iterate.json\n",
      "saving metrics to train.padl/0.metrics.json\n",
      "saving iteration to train.padl/0.iteration.json\n",
      "saving epoch to train.padl/0.epoch.json\n",
      "saving torch module to train.padl/10.pt\n",
      "TRAIN epoch 0; iteration 1; loss 2.1678245067596436;\n",
      "TRAIN epoch 0; iteration 2; loss 2.0870473384857178;\n",
      "TRAIN epoch 0; iteration 3; loss 1.8864717483520508;\n",
      "TRAIN epoch 0; iteration 4; loss 1.7097222805023193;\n",
      "TRAIN epoch 0; iteration 5; loss 1.7313315868377686;\n",
      "TRAIN epoch 0; iteration 6; loss 1.6161465644836426;\n",
      "TRAIN epoch 0; iteration 7; loss 1.4628639221191406;\n",
      "TRAIN epoch 0; iteration 8; loss 1.2240945100784302;\n",
      "TRAIN epoch 0; iteration 9; loss 1.2035975456237793;\n",
      "TRAIN epoch 0; iteration 10; loss 1.0419020652770996;\n",
      "TRAIN epoch 0; iteration 11; loss 0.9660502076148987;\n",
      "TRAIN epoch 0; iteration 12; loss 0.962110161781311;\n",
      "TRAIN epoch 0; iteration 13; loss 0.8199653625488281;\n",
      "TRAIN epoch 0; iteration 14; loss 0.6595463752746582;\n",
      "TRAIN epoch 0; iteration 15; loss 0.6989786624908447;\n",
      "TRAIN epoch 0; iteration 16; loss 0.5917927026748657;\n",
      "TRAIN epoch 0; iteration 17; loss 0.37667709589004517;\n",
      "TRAIN epoch 0; iteration 18; loss 0.42024287581443787;\n",
      "TRAIN epoch 0; iteration 19; loss 0.4431636929512024;\n",
      "TRAIN epoch 0; iteration 20; loss 0.5163788795471191;\n",
      "TRAIN epoch 0; iteration 21; loss 0.34609293937683105;\n",
      "TRAIN epoch 0; iteration 22; loss 0.3856998682022095;\n",
      "TRAIN epoch 0; iteration 23; loss 0.35558629035949707;\n",
      "TRAIN epoch 0; iteration 24; loss 0.46402424573898315;\n",
      "TRAIN epoch 0; iteration 25; loss 0.24797610938549042;\n",
      "TRAIN epoch 0; iteration 26; loss 0.4257711172103882;\n",
      "TRAIN epoch 0; iteration 27; loss 0.4554227590560913;\n",
      "TRAIN epoch 0; iteration 28; loss 0.2752155065536499;\n",
      "TRAIN epoch 0; iteration 29; loss 0.3865244388580322;\n",
      "TRAIN epoch 0; iteration 30; loss 0.3763803243637085;\n",
      "TRAIN epoch 0; iteration 31; loss 0.17167985439300537;\n",
      "TRAIN epoch 0; iteration 32; loss 0.3134394586086273;\n",
      "TRAIN epoch 0; iteration 33; loss 0.34580859541893005;\n",
      "TRAIN epoch 0; iteration 34; loss 0.23178589344024658;\n",
      "TRAIN epoch 0; iteration 35; loss 0.24932131171226501;\n",
      "TRAIN epoch 0; iteration 36; loss 0.3658054769039154;\n",
      "TRAIN epoch 0; iteration 37; loss 0.3711235523223877;\n",
      "TRAIN epoch 0; iteration 38; loss 0.18664899468421936;\n",
      "TRAIN epoch 0; iteration 39; loss 0.17260321974754333;\n",
      "TRAIN epoch 0; iteration 40; loss 0.17655795812606812;\n",
      "TRAIN epoch 0; iteration 41; loss 0.33908936381340027;\n",
      "TRAIN epoch 0; iteration 42; loss 0.2697533369064331;\n",
      "TRAIN epoch 0; iteration 43; loss 0.16594892740249634;\n",
      "TRAIN epoch 0; iteration 44; loss 0.16038621962070465;\n",
      "TRAIN epoch 0; iteration 45; loss 0.18590374290943146;\n",
      "TRAIN epoch 0; iteration 46; loss 0.32215890288352966;\n",
      "TRAIN epoch 0; iteration 47; loss 0.2348940521478653;\n",
      "TRAIN epoch 0; iteration 48; loss 0.24067440629005432;\n",
      "TRAIN epoch 0; iteration 49; loss 0.29026147723197937;\n",
      "TRAIN epoch 0; iteration 50; loss 0.16840621829032898;\n",
      "TRAIN epoch 0; iteration 51; loss 0.2790936231613159;\n",
      "TRAIN epoch 0; iteration 52; loss 0.13604891300201416;\n",
      "TRAIN epoch 0; iteration 53; loss 0.22526270151138306;\n",
      "TRAIN epoch 0; iteration 54; loss 0.09304284304380417;\n",
      "TRAIN epoch 0; iteration 55; loss 0.22476692497730255;\n",
      "TRAIN epoch 0; iteration 56; loss 0.23052987456321716;\n",
      "TRAIN epoch 0; iteration 57; loss 0.29769858717918396;\n",
      "TRAIN epoch 0; iteration 58; loss 0.20388153195381165;\n",
      "TRAIN epoch 0; iteration 59; loss 0.14341723918914795;\n",
      "TRAIN epoch 0; iteration 60; loss 0.0758812204003334;\n",
      "TRAIN epoch 0; iteration 61; loss 0.1559232920408249;\n",
      "TRAIN epoch 0; iteration 62; loss 0.2115374505519867;\n",
      "TRAIN epoch 0; iteration 63; loss 0.16210141777992249;\n",
      "TRAIN epoch 0; iteration 64; loss 0.1990770697593689;\n",
      "TRAIN epoch 0; iteration 65; loss 0.17182256281375885;\n",
      "TRAIN epoch 0; iteration 66; loss 0.1684844046831131;\n",
      "TRAIN epoch 0; iteration 67; loss 0.07842771708965302;\n",
      "TRAIN epoch 0; iteration 68; loss 0.3259223997592926;\n",
      "TRAIN epoch 0; iteration 69; loss 0.22925642132759094;\n",
      "TRAIN epoch 0; iteration 70; loss 0.23217597603797913;\n",
      "TRAIN epoch 0; iteration 71; loss 0.18609116971492767;\n",
      "TRAIN epoch 0; iteration 72; loss 0.31906670331954956;\n",
      "TRAIN epoch 0; iteration 73; loss 0.1958039104938507;\n",
      "TRAIN epoch 0; iteration 74; loss 0.09275498986244202;\n",
      "TRAIN epoch 0; iteration 75; loss 0.18094977736473083;\n",
      "TRAIN epoch 0; iteration 76; loss 0.13692861795425415;\n",
      "TRAIN epoch 0; iteration 77; loss 0.24204681813716888;\n",
      "TRAIN epoch 0; iteration 78; loss 0.2020159512758255;\n",
      "TRAIN epoch 0; iteration 79; loss 0.22322402894496918;\n",
      "TRAIN epoch 0; iteration 80; loss 0.0951668992638588;\n",
      "TRAIN epoch 0; iteration 81; loss 0.1854434758424759;\n",
      "TRAIN epoch 0; iteration 82; loss 0.29489749670028687;\n",
      "TRAIN epoch 0; iteration 83; loss 0.07929804176092148;\n",
      "TRAIN epoch 0; iteration 84; loss 0.23132570087909698;\n",
      "TRAIN epoch 0; iteration 85; loss 0.0822729840874672;\n",
      "TRAIN epoch 0; iteration 86; loss 0.275155633687973;\n",
      "TRAIN epoch 0; iteration 87; loss 0.353028804063797;\n",
      "TRAIN epoch 0; iteration 88; loss 0.23331478238105774;\n",
      "TRAIN epoch 0; iteration 89; loss 0.2228362113237381;\n",
      "TRAIN epoch 0; iteration 90; loss 0.0862283781170845;\n",
      "TRAIN epoch 0; iteration 91; loss 0.12029167264699936;\n",
      "TRAIN epoch 0; iteration 92; loss 0.16530835628509521;\n",
      "TRAIN epoch 0; iteration 93; loss 0.15979930758476257;\n",
      "TRAIN epoch 0; iteration 94; loss 0.3293011784553528;\n",
      "TRAIN epoch 0; iteration 95; loss 0.10485421866178513;\n",
      "TRAIN epoch 0; iteration 96; loss 0.1661474108695984;\n",
      "TRAIN epoch 0; iteration 97; loss 0.13764885067939758;\n",
      "TRAIN epoch 0; iteration 98; loss 0.12313870340585709;\n",
      "TRAIN epoch 0; iteration 99; loss 0.0952247604727745;\n",
      "TRAIN epoch 0; iteration 100; loss 0.1315847486257553;\n",
      "TRAIN epoch 0; iteration 100;loss: 0.10116876832209527;accuracy: 0.9686;\n",
      "saving optimizer state to train.padl/0.pt\n",
      "saving iterate args to train.padl/0.iterate.json\n",
      "saving metrics to train.padl/0.metrics.json\n",
      "saving iteration to train.padl/0.iteration.json\n",
      "saving epoch to train.padl/0.epoch.json\n",
      "saving torch module to train.padl/10.pt\n",
      "TRAIN epoch 0; iteration 101; loss 0.11272790282964706;\n",
      "TRAIN epoch 0; iteration 102; loss 0.43969982862472534;\n",
      "TRAIN epoch 0; iteration 103; loss 0.11125318706035614;\n",
      "TRAIN epoch 0; iteration 104; loss 0.05971576273441315;\n",
      "TRAIN epoch 0; iteration 105; loss 0.08242999762296677;\n",
      "TRAIN epoch 0; iteration 106; loss 0.061636414378881454;\n",
      "TRAIN epoch 0; iteration 107; loss 0.17337943613529205;\n",
      "TRAIN epoch 0; iteration 108; loss 0.19649940729141235;\n",
      "TRAIN epoch 0; iteration 109; loss 0.29866984486579895;\n",
      "TRAIN epoch 0; iteration 110; loss 0.07029516994953156;\n",
      "TRAIN epoch 0; iteration 111; loss 0.04058866947889328;\n",
      "TRAIN epoch 0; iteration 112; loss 0.07790020108222961;\n",
      "TRAIN epoch 0; iteration 113; loss 0.09546177089214325;\n",
      "TRAIN epoch 0; iteration 114; loss 0.08663362264633179;\n",
      "TRAIN epoch 0; iteration 115; loss 0.20622293651103973;\n",
      "TRAIN epoch 0; iteration 116; loss 0.13049133121967316;\n",
      "TRAIN epoch 0; iteration 117; loss 0.1835959553718567;\n",
      "TRAIN epoch 0; iteration 118; loss 0.1236964613199234;\n",
      "TRAIN epoch 0; iteration 119; loss 0.05819791182875633;\n",
      "TRAIN epoch 0; iteration 120; loss 0.10276759415864944;\n",
      "TRAIN epoch 0; iteration 121; loss 0.07178846746683121;\n",
      "TRAIN epoch 0; iteration 122; loss 0.13797622919082642;\n",
      "TRAIN epoch 0; iteration 123; loss 0.15315227210521698;\n",
      "TRAIN epoch 0; iteration 124; loss 0.16391469538211823;\n",
      "TRAIN epoch 0; iteration 125; loss 0.23842306435108185;\n",
      "TRAIN epoch 0; iteration 126; loss 0.14463400840759277;\n",
      "TRAIN epoch 0; iteration 127; loss 0.05616520717740059;\n",
      "TRAIN epoch 0; iteration 128; loss 0.08809247612953186;\n",
      "TRAIN epoch 0; iteration 129; loss 0.16604289412498474;\n",
      "TRAIN epoch 0; iteration 130; loss 0.19109362363815308;\n",
      "TRAIN epoch 0; iteration 131; loss 0.26581209897994995;\n",
      "TRAIN epoch 0; iteration 132; loss 0.0754115879535675;\n",
      "TRAIN epoch 0; iteration 133; loss 0.12739448249340057;\n",
      "TRAIN epoch 0; iteration 134; loss 0.07799815386533737;\n",
      "TRAIN epoch 0; iteration 135; loss 0.06757393479347229;\n",
      "TRAIN epoch 0; iteration 136; loss 0.16011224687099457;\n",
      "TRAIN epoch 0; iteration 137; loss 0.13969218730926514;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 0; iteration 138; loss 0.08682747185230255;\n",
      "TRAIN epoch 0; iteration 139; loss 0.11181879043579102;\n",
      "TRAIN epoch 0; iteration 140; loss 0.12912163138389587;\n",
      "TRAIN epoch 0; iteration 141; loss 0.05694926157593727;\n",
      "TRAIN epoch 0; iteration 142; loss 0.12077995389699936;\n",
      "TRAIN epoch 0; iteration 143; loss 0.15509873628616333;\n",
      "TRAIN epoch 0; iteration 144; loss 0.04324187710881233;\n",
      "TRAIN epoch 0; iteration 145; loss 0.10938926786184311;\n",
      "TRAIN epoch 0; iteration 146; loss 0.141669362783432;\n",
      "TRAIN epoch 0; iteration 147; loss 0.2420688271522522;\n",
      "TRAIN epoch 0; iteration 148; loss 0.09334823489189148;\n",
      "TRAIN epoch 0; iteration 149; loss 0.08134731650352478;\n",
      "TRAIN epoch 0; iteration 150; loss 0.04520973563194275;\n",
      "TRAIN epoch 0; iteration 151; loss 0.15030327439308167;\n",
      "TRAIN epoch 0; iteration 152; loss 0.08089284598827362;\n",
      "TRAIN epoch 0; iteration 153; loss 0.06749270856380463;\n",
      "TRAIN epoch 0; iteration 154; loss 0.11044564843177795;\n",
      "TRAIN epoch 0; iteration 155; loss 0.0443129763007164;\n",
      "TRAIN epoch 0; iteration 156; loss 0.048511553555727005;\n",
      "TRAIN epoch 0; iteration 157; loss 0.18251514434814453;\n",
      "TRAIN epoch 0; iteration 158; loss 0.157819464802742;\n",
      "TRAIN epoch 0; iteration 159; loss 0.16248919069766998;\n",
      "TRAIN epoch 0; iteration 160; loss 0.08797528594732285;\n",
      "TRAIN epoch 0; iteration 161; loss 0.18223749101161957;\n",
      "TRAIN epoch 0; iteration 162; loss 0.05919502303004265;\n",
      "TRAIN epoch 0; iteration 163; loss 0.14332786202430725;\n",
      "TRAIN epoch 0; iteration 164; loss 0.057043179869651794;\n",
      "TRAIN epoch 0; iteration 165; loss 0.12494295835494995;\n",
      "TRAIN epoch 0; iteration 166; loss 0.07258302718400955;\n",
      "TRAIN epoch 0; iteration 167; loss 0.09887248277664185;\n",
      "TRAIN epoch 0; iteration 168; loss 0.09284783154726028;\n",
      "TRAIN epoch 0; iteration 169; loss 0.14451321959495544;\n",
      "TRAIN epoch 0; iteration 170; loss 0.10235259681940079;\n",
      "TRAIN epoch 0; iteration 171; loss 0.10794823616743088;\n",
      "TRAIN epoch 0; iteration 172; loss 0.07218465209007263;\n",
      "TRAIN epoch 0; iteration 173; loss 0.020174356177449226;\n",
      "TRAIN epoch 0; iteration 174; loss 0.12852627038955688;\n",
      "TRAIN epoch 0; iteration 175; loss 0.11402773857116699;\n",
      "TRAIN epoch 0; iteration 176; loss 0.04403238371014595;\n",
      "TRAIN epoch 0; iteration 177; loss 0.12476921081542969;\n",
      "TRAIN epoch 0; iteration 178; loss 0.1363932192325592;\n",
      "TRAIN epoch 0; iteration 179; loss 0.07400540262460709;\n",
      "TRAIN epoch 0; iteration 180; loss 0.06336385011672974;\n",
      "TRAIN epoch 0; iteration 181; loss 0.07338938862085342;\n",
      "TRAIN epoch 0; iteration 182; loss 0.03187621384859085;\n",
      "TRAIN epoch 0; iteration 183; loss 0.15487056970596313;\n",
      "TRAIN epoch 0; iteration 184; loss 0.09276917576789856;\n",
      "TRAIN epoch 0; iteration 185; loss 0.08707783371210098;\n",
      "TRAIN epoch 0; iteration 186; loss 0.032925739884376526;\n",
      "TRAIN epoch 0; iteration 187; loss 0.05326218158006668;\n",
      "TRAIN epoch 0; iteration 188; loss 0.02798870950937271;\n",
      "TRAIN epoch 0; iteration 189; loss 0.017549145966768265;\n",
      "TRAIN epoch 0; iteration 190; loss 0.05642170086503029;\n",
      "TRAIN epoch 0; iteration 191; loss 0.10627630352973938;\n",
      "TRAIN epoch 0; iteration 192; loss 0.12488868832588196;\n",
      "TRAIN epoch 0; iteration 193; loss 0.11298327147960663;\n",
      "TRAIN epoch 0; iteration 194; loss 0.06736557185649872;\n",
      "TRAIN epoch 0; iteration 195; loss 0.11877450346946716;\n",
      "TRAIN epoch 0; iteration 196; loss 0.016929050907492638;\n",
      "TRAIN epoch 0; iteration 197; loss 0.012428198009729385;\n",
      "TRAIN epoch 0; iteration 198; loss 0.05581240728497505;\n",
      "TRAIN epoch 0; iteration 199; loss 0.10691919177770615;\n",
      "TRAIN epoch 0; iteration 200; loss 0.09073648601770401;\n",
      "TRAIN epoch 0; iteration 200;loss: 0.05496359162556473;accuracy: 0.9817;\n",
      "saving optimizer state to train.padl/0.pt\n",
      "saving iterate args to train.padl/0.iterate.json\n",
      "saving metrics to train.padl/0.metrics.json\n",
      "saving iteration to train.padl/0.iteration.json\n",
      "saving epoch to train.padl/0.epoch.json\n",
      "saving torch module to train.padl/10.pt\n",
      "TRAIN epoch 0; iteration 201; loss 0.14168047904968262;\n",
      "TRAIN epoch 0; iteration 202; loss 0.11014597117900848;\n",
      "TRAIN epoch 0; iteration 203; loss 0.06033938750624657;\n",
      "TRAIN epoch 0; iteration 204; loss 0.013090210035443306;\n",
      "TRAIN epoch 0; iteration 205; loss 0.05793692171573639;\n",
      "TRAIN epoch 0; iteration 206; loss 0.0905543640255928;\n",
      "TRAIN epoch 0; iteration 207; loss 0.1064903661608696;\n",
      "TRAIN epoch 0; iteration 208; loss 0.08371469378471375;\n",
      "TRAIN epoch 0; iteration 209; loss 0.044433802366256714;\n",
      "TRAIN epoch 0; iteration 210; loss 0.04352729022502899;\n",
      "TRAIN epoch 0; iteration 211; loss 0.14930562674999237;\n",
      "TRAIN epoch 0; iteration 212; loss 0.024443505331873894;\n",
      "TRAIN epoch 0; iteration 213; loss 0.08571215718984604;\n",
      "TRAIN epoch 0; iteration 214; loss 0.04938215762376785;\n",
      "TRAIN epoch 0; iteration 215; loss 0.04022805765271187;\n",
      "TRAIN epoch 0; iteration 216; loss 0.09906291216611862;\n",
      "TRAIN epoch 0; iteration 217; loss 0.04328646510839462;\n",
      "TRAIN epoch 0; iteration 218; loss 0.09418442100286484;\n",
      "TRAIN epoch 0; iteration 219; loss 0.08124500513076782;\n",
      "TRAIN epoch 0; iteration 220; loss 0.026198742911219597;\n",
      "TRAIN epoch 0; iteration 221; loss 0.11608214676380157;\n",
      "TRAIN epoch 0; iteration 222; loss 0.20762185752391815;\n",
      "TRAIN epoch 0; iteration 223; loss 0.024461492896080017;\n",
      "TRAIN epoch 0; iteration 224; loss 0.09684033691883087;\n",
      "TRAIN epoch 0; iteration 225; loss 0.08005382865667343;\n",
      "TRAIN epoch 0; iteration 226; loss 0.0871012881398201;\n",
      "TRAIN epoch 0; iteration 227; loss 0.08154620975255966;\n",
      "TRAIN epoch 0; iteration 228; loss 0.030347168445587158;\n",
      "TRAIN epoch 0; iteration 229; loss 0.013771910220384598;\n",
      "TRAIN epoch 0; iteration 230; loss 0.11099543422460556;\n",
      "TRAIN epoch 0; iteration 231; loss 0.04405161365866661;\n",
      "TRAIN epoch 0; iteration 232; loss 0.049069877713918686;\n",
      "TRAIN epoch 0; iteration 233; loss 0.021733855828642845;\n",
      "TRAIN epoch 0; iteration 234; loss 0.0609208308160305;\n",
      "TRAIN epoch 0; iteration 235; loss 0.08143844455480576;\n",
      "TRAIN epoch 0; iteration 236; loss 0.04326160252094269;\n",
      "TRAIN epoch 0; iteration 237; loss 0.1304491013288498;\n",
      "TRAIN epoch 0; iteration 238; loss 0.11979991942644119;\n",
      "TRAIN epoch 0; iteration 239; loss 0.1168723851442337;\n",
      "TRAIN epoch 0; iteration 240; loss 0.05693115293979645;\n",
      "TRAIN epoch 0; iteration 241; loss 0.03472302109003067;\n",
      "TRAIN epoch 0; iteration 242; loss 0.04920231178402901;\n",
      "TRAIN epoch 0; iteration 243; loss 0.007380682975053787;\n",
      "TRAIN epoch 0; iteration 244; loss 0.07458588480949402;\n",
      "TRAIN epoch 0; iteration 245; loss 0.0897860899567604;\n",
      "TRAIN epoch 0; iteration 246; loss 0.0729759931564331;\n",
      "TRAIN epoch 0; iteration 247; loss 0.13130183517932892;\n",
      "TRAIN epoch 0; iteration 248; loss 0.024798981845378876;\n",
      "TRAIN epoch 0; iteration 249; loss 0.07032088935375214;\n",
      "TRAIN epoch 0; iteration 250; loss 0.020517654716968536;\n",
      "TRAIN epoch 0; iteration 251; loss 0.03235113248229027;\n",
      "TRAIN epoch 0; iteration 252; loss 0.0744524896144867;\n",
      "TRAIN epoch 0; iteration 253; loss 0.04806464537978172;\n",
      "TRAIN epoch 0; iteration 254; loss 0.042168378829956055;\n",
      "TRAIN epoch 0; iteration 255; loss 0.1426635980606079;\n",
      "TRAIN epoch 0; iteration 256; loss 0.134879931807518;\n",
      "TRAIN epoch 0; iteration 257; loss 0.06618668884038925;\n",
      "TRAIN epoch 0; iteration 258; loss 0.07048168778419495;\n",
      "TRAIN epoch 0; iteration 259; loss 0.06686022132635117;\n",
      "TRAIN epoch 0; iteration 260; loss 0.03876316547393799;\n",
      "TRAIN epoch 0; iteration 261; loss 0.022107960656285286;\n",
      "TRAIN epoch 0; iteration 262; loss 0.03002946823835373;\n",
      "TRAIN epoch 0; iteration 263; loss 0.12367435544729233;\n",
      "TRAIN epoch 0; iteration 264; loss 0.09386025369167328;\n",
      "TRAIN epoch 0; iteration 265; loss 0.24076873064041138;\n",
      "TRAIN epoch 0; iteration 266; loss 0.2461051493883133;\n",
      "TRAIN epoch 0; iteration 267; loss 0.23072479665279388;\n",
      "TRAIN epoch 0; iteration 268; loss 0.09715240448713303;\n",
      "TRAIN epoch 0; iteration 269; loss 0.048415083438158035;\n",
      "TRAIN epoch 0; iteration 270; loss 0.03600672259926796;\n",
      "TRAIN epoch 0; iteration 271; loss 0.14805182814598083;\n",
      "TRAIN epoch 0; iteration 272; loss 0.1757696568965912;\n",
      "TRAIN epoch 0; iteration 273; loss 0.03678172454237938;\n",
      "TRAIN epoch 0; iteration 274; loss 0.06464079022407532;\n",
      "TRAIN epoch 0; iteration 275; loss 0.09388551115989685;\n",
      "TRAIN epoch 0; iteration 276; loss 0.1687539517879486;\n",
      "TRAIN epoch 0; iteration 277; loss 0.06955169886350632;\n",
      "TRAIN epoch 0; iteration 278; loss 0.0882553830742836;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 0; iteration 279; loss 0.02718876302242279;\n",
      "TRAIN epoch 0; iteration 280; loss 0.06278645247220993;\n",
      "TRAIN epoch 0; iteration 281; loss 0.08634012192487717;\n",
      "TRAIN epoch 0; iteration 282; loss 0.0656232014298439;\n",
      "TRAIN epoch 0; iteration 283; loss 0.19936466217041016;\n",
      "TRAIN epoch 0; iteration 284; loss 0.0804126039147377;\n",
      "TRAIN epoch 0; iteration 285; loss 0.11205287277698517;\n",
      "TRAIN epoch 0; iteration 286; loss 0.14829227328300476;\n",
      "TRAIN epoch 0; iteration 287; loss 0.161542609333992;\n",
      "TRAIN epoch 0; iteration 288; loss 0.031323764473199844;\n",
      "TRAIN epoch 0; iteration 289; loss 0.05592922121286392;\n",
      "TRAIN epoch 0; iteration 290; loss 0.011089359410107136;\n",
      "TRAIN epoch 0; iteration 291; loss 0.07573393732309341;\n",
      "TRAIN epoch 0; iteration 292; loss 0.08053582161664963;\n",
      "TRAIN epoch 0; iteration 293; loss 0.13012762367725372;\n",
      "TRAIN epoch 0; iteration 294; loss 0.0680394247174263;\n",
      "TRAIN epoch 0; iteration 295; loss 0.054759763181209564;\n",
      "TRAIN epoch 0; iteration 296; loss 0.01897226646542549;\n",
      "TRAIN epoch 0; iteration 297; loss 0.10251675546169281;\n",
      "TRAIN epoch 0; iteration 298; loss 0.1449131965637207;\n",
      "TRAIN epoch 0; iteration 299; loss 0.11182998865842819;\n",
      "TRAIN epoch 0; iteration 300; loss 0.0844791904091835;\n",
      "TRAIN epoch 0; iteration 300;loss: 0.053904116444318785;accuracy: 0.9839;\n",
      "saving optimizer state to train.padl/0.pt\n",
      "saving iterate args to train.padl/0.iterate.json\n",
      "saving metrics to train.padl/0.metrics.json\n",
      "saving iteration to train.padl/0.iteration.json\n",
      "saving epoch to train.padl/0.epoch.json\n",
      "saving torch module to train.padl/10.pt\n",
      "TRAIN epoch 0; iteration 301; loss 0.08562587946653366;\n",
      "TRAIN epoch 0; iteration 302; loss 0.010586929507553577;\n",
      "TRAIN epoch 0; iteration 303; loss 0.02510601095855236;\n",
      "TRAIN epoch 0; iteration 304; loss 0.027963660657405853;\n",
      "TRAIN epoch 0; iteration 305; loss 0.06054319813847542;\n",
      "TRAIN epoch 0; iteration 306; loss 0.14792925119400024;\n",
      "TRAIN epoch 0; iteration 307; loss 0.027645990252494812;\n",
      "TRAIN epoch 0; iteration 308; loss 0.06713639944791794;\n",
      "TRAIN epoch 0; iteration 309; loss 0.12183436751365662;\n",
      "TRAIN epoch 0; iteration 310; loss 0.06086193025112152;\n",
      "TRAIN epoch 0; iteration 311; loss 0.09568360447883606;\n",
      "TRAIN epoch 0; iteration 312; loss 0.05122196674346924;\n",
      "TRAIN epoch 0; iteration 313; loss 0.13817229866981506;\n",
      "TRAIN epoch 0; iteration 314; loss 0.12469588965177536;\n",
      "TRAIN epoch 0; iteration 315; loss 0.15258008241653442;\n",
      "TRAIN epoch 0; iteration 316; loss 0.055020011961460114;\n",
      "TRAIN epoch 0; iteration 317; loss 0.16012126207351685;\n",
      "TRAIN epoch 0; iteration 318; loss 0.08535150438547134;\n",
      "TRAIN epoch 0; iteration 319; loss 0.040018998086452484;\n",
      "TRAIN epoch 0; iteration 320; loss 0.1127178743481636;\n",
      "TRAIN epoch 0; iteration 321; loss 0.1240205317735672;\n",
      "TRAIN epoch 0; iteration 322; loss 0.053856391459703445;\n",
      "TRAIN epoch 0; iteration 323; loss 0.1032295674085617;\n",
      "TRAIN epoch 0; iteration 324; loss 0.21577966213226318;\n",
      "TRAIN epoch 0; iteration 325; loss 0.06460396945476532;\n",
      "TRAIN epoch 0; iteration 326; loss 0.01749506965279579;\n",
      "TRAIN epoch 0; iteration 327; loss 0.14556455612182617;\n",
      "TRAIN epoch 0; iteration 328; loss 0.14241990447044373;\n",
      "TRAIN epoch 0; iteration 329; loss 0.011234313249588013;\n",
      "TRAIN epoch 0; iteration 330; loss 0.07753045856952667;\n",
      "TRAIN epoch 0; iteration 331; loss 0.09329180419445038;\n",
      "TRAIN epoch 0; iteration 332; loss 0.03974214196205139;\n",
      "TRAIN epoch 0; iteration 333; loss 0.11489218473434448;\n",
      "TRAIN epoch 0; iteration 334; loss 0.13338898122310638;\n",
      "TRAIN epoch 0; iteration 335; loss 0.08791358768939972;\n",
      "TRAIN epoch 0; iteration 336; loss 0.0716906487941742;\n",
      "TRAIN epoch 0; iteration 337; loss 0.09099549055099487;\n",
      "TRAIN epoch 0; iteration 338; loss 0.00885956734418869;\n",
      "TRAIN epoch 0; iteration 339; loss 0.01505110040307045;\n",
      "TRAIN epoch 0; iteration 340; loss 0.14395831525325775;\n",
      "TRAIN epoch 0; iteration 341; loss 0.021014682948589325;\n",
      "TRAIN epoch 0; iteration 342; loss 0.015488464385271072;\n",
      "TRAIN epoch 0; iteration 343; loss 0.06333637237548828;\n",
      "TRAIN epoch 0; iteration 344; loss 0.14958922564983368;\n",
      "TRAIN epoch 0; iteration 345; loss 0.07102932780981064;\n",
      "TRAIN epoch 0; iteration 346; loss 0.134131520986557;\n",
      "TRAIN epoch 0; iteration 347; loss 0.1703755408525467;\n",
      "TRAIN epoch 0; iteration 348; loss 0.14420701563358307;\n",
      "TRAIN epoch 0; iteration 349; loss 0.13793161511421204;\n",
      "TRAIN epoch 0; iteration 350; loss 0.029426362365484238;\n",
      "TRAIN epoch 0; iteration 351; loss 0.08983876556158066;\n",
      "TRAIN epoch 0; iteration 352; loss 0.03772454708814621;\n",
      "TRAIN epoch 0; iteration 353; loss 0.0807720273733139;\n",
      "TRAIN epoch 0; iteration 354; loss 0.15423598885536194;\n",
      "TRAIN epoch 0; iteration 355; loss 0.011666208505630493;\n",
      "TRAIN epoch 0; iteration 356; loss 0.0962112620472908;\n",
      "TRAIN epoch 0; iteration 357; loss 0.053982142359018326;\n",
      "TRAIN epoch 0; iteration 358; loss 0.12117113173007965;\n",
      "TRAIN epoch 0; iteration 359; loss 0.06496568769216537;\n",
      "TRAIN epoch 0; iteration 360; loss 0.19117581844329834;\n",
      "TRAIN epoch 0; iteration 361; loss 0.08327735215425491;\n",
      "TRAIN epoch 0; iteration 362; loss 0.03888685628771782;\n",
      "TRAIN epoch 0; iteration 363; loss 0.03841588646173477;\n",
      "TRAIN epoch 0; iteration 364; loss 0.09665925055742264;\n",
      "TRAIN epoch 0; iteration 365; loss 0.023595061153173447;\n",
      "TRAIN epoch 0; iteration 366; loss 0.02013416774570942;\n",
      "TRAIN epoch 0; iteration 367; loss 0.15082688629627228;\n",
      "quitting training...\n"
     ]
    }
   ],
   "source": [
    "metric_data = [x[0] for x in valid_data]\n",
    "ground_truth = [x[1] for x in valid_data]\n",
    "\n",
    "try:\n",
    "    t.train(train_data, 'train.padl', valid_data=valid_data,\n",
    "            save_interval=100, batch_size=100, metric_data=metric_data, ground_truth=ground_truth)\n",
    "except KeyboardInterrupt:\n",
    "    print('quitting training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the model has been trained, the entire trainer corresponding to the best model\n",
    "may be reloaded. This works in a completely new session, and without additional setup\n",
    "or imports."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309d1dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading optimizer from state-dict\n",
      "loading iterate args\n",
      "loading metrics\n",
      "loading iteration\n",
      "loading epoch\n",
      "loading torch module from train.padl/10.pt\n"
     ]
    }
   ],
   "source": [
    "from padl import load\n",
    "\n",
    "s = load('train.padl')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may simply continue training where you left off, the weights\n",
    "and the state of the optimizer will continue at the point they were at:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6482cf35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 1; iteration 301; loss 0.10702324658632278;\n",
      "TRAIN epoch 1; iteration 302; loss 0.1330687254667282;\n",
      "TRAIN epoch 1; iteration 303; loss 0.06240423768758774;\n",
      "TRAIN epoch 1; iteration 304; loss 0.01103877741843462;\n",
      "TRAIN epoch 1; iteration 305; loss 0.21846503019332886;\n",
      "TRAIN epoch 1; iteration 306; loss 0.16267511248588562;\n",
      "TRAIN epoch 1; iteration 307; loss 0.09883277863264084;\n",
      "TRAIN epoch 1; iteration 308; loss 0.08944755792617798;\n",
      "TRAIN epoch 1; iteration 309; loss 0.10024800151586533;\n",
      "TRAIN epoch 1; iteration 310; loss 0.16785916686058044;\n",
      "TRAIN epoch 1; iteration 311; loss 0.1738307923078537;\n",
      "TRAIN epoch 1; iteration 312; loss 0.024245362728834152;\n",
      "TRAIN epoch 1; iteration 313; loss 0.11180571466684341;\n",
      "TRAIN epoch 1; iteration 314; loss 0.10000386834144592;\n",
      "TRAIN epoch 1; iteration 315; loss 0.07730059325695038;\n",
      "TRAIN epoch 1; iteration 316; loss 0.07188447564840317;\n",
      "TRAIN epoch 1; iteration 317; loss 0.12086562067270279;\n",
      "TRAIN epoch 1; iteration 318; loss 0.06348782032728195;\n",
      "TRAIN epoch 1; iteration 319; loss 0.009607718326151371;\n",
      "TRAIN epoch 1; iteration 320; loss 0.10333554446697235;\n",
      "TRAIN epoch 1; iteration 321; loss 0.029280077666044235;\n",
      "TRAIN epoch 1; iteration 322; loss 0.026811547577381134;\n",
      "TRAIN epoch 1; iteration 323; loss 0.028502725064754486;\n",
      "TRAIN epoch 1; iteration 324; loss 0.019281545653939247;\n",
      "TRAIN epoch 1; iteration 325; loss 0.10309559106826782;\n",
      "TRAIN epoch 1; iteration 326; loss 0.07569441944360733;\n",
      "TRAIN epoch 1; iteration 327; loss 0.19350223243236542;\n",
      "TRAIN epoch 1; iteration 328; loss 0.0892094224691391;\n",
      "TRAIN epoch 1; iteration 329; loss 0.03938734531402588;\n",
      "TRAIN epoch 1; iteration 330; loss 0.05652375519275665;\n",
      "TRAIN epoch 1; iteration 331; loss 0.039668552577495575;\n",
      "TRAIN epoch 1; iteration 332; loss 0.009148255921900272;\n",
      "TRAIN epoch 1; iteration 333; loss 0.01506852637976408;\n",
      "TRAIN epoch 1; iteration 334; loss 0.015053256414830685;\n",
      "TRAIN epoch 1; iteration 335; loss 0.02382619120180607;\n",
      "TRAIN epoch 1; iteration 336; loss 0.1285863071680069;\n",
      "TRAIN epoch 1; iteration 337; loss 0.13533702492713928;\n",
      "TRAIN epoch 1; iteration 338; loss 0.08146333694458008;\n",
      "TRAIN epoch 1; iteration 339; loss 0.0373934768140316;\n",
      "TRAIN epoch 1; iteration 340; loss 0.06117761507630348;\n",
      "TRAIN epoch 1; iteration 341; loss 0.06862699240446091;\n",
      "TRAIN epoch 1; iteration 342; loss 0.049996498972177505;\n",
      "TRAIN epoch 1; iteration 343; loss 0.08298758417367935;\n",
      "TRAIN epoch 1; iteration 344; loss 0.10411408543586731;\n",
      "TRAIN epoch 1; iteration 345; loss 0.11232037842273712;\n",
      "TRAIN epoch 1; iteration 346; loss 0.02775951474905014;\n",
      "TRAIN epoch 1; iteration 347; loss 0.08102662861347198;\n",
      "TRAIN epoch 1; iteration 348; loss 0.012646756134927273;\n",
      "TRAIN epoch 1; iteration 349; loss 0.03796776756644249;\n",
      "TRAIN epoch 1; iteration 350; loss 0.09059232473373413;\n",
      "TRAIN epoch 1; iteration 351; loss 0.04619588330388069;\n",
      "TRAIN epoch 1; iteration 352; loss 0.14335700869560242;\n",
      "TRAIN epoch 1; iteration 353; loss 0.08489537984132767;\n",
      "TRAIN epoch 1; iteration 354; loss 0.07328099012374878;\n",
      "TRAIN epoch 1; iteration 355; loss 0.05031093582510948;\n",
      "TRAIN epoch 1; iteration 356; loss 0.06869938224554062;\n",
      "TRAIN epoch 1; iteration 357; loss 0.0824260413646698;\n",
      "TRAIN epoch 1; iteration 358; loss 0.13870035111904144;\n",
      "TRAIN epoch 1; iteration 359; loss 0.08454002439975739;\n",
      "TRAIN epoch 1; iteration 360; loss 0.02821890078485012;\n",
      "TRAIN epoch 1; iteration 361; loss 0.04009650647640228;\n",
      "TRAIN epoch 1; iteration 362; loss 0.0504838190972805;\n",
      "TRAIN epoch 1; iteration 363; loss 0.09292346984148026;\n",
      "TRAIN epoch 1; iteration 364; loss 0.06921975314617157;\n",
      "TRAIN epoch 1; iteration 365; loss 0.06841212511062622;\n",
      "TRAIN epoch 1; iteration 366; loss 0.02636151760816574;\n",
      "TRAIN epoch 1; iteration 367; loss 0.028160901740193367;\n",
      "TRAIN epoch 1; iteration 368; loss 0.08945974707603455;\n",
      "TRAIN epoch 1; iteration 369; loss 0.10691098868846893;\n",
      "TRAIN epoch 1; iteration 370; loss 0.11719009280204773;\n",
      "TRAIN epoch 1; iteration 371; loss 0.08440092206001282;\n",
      "TRAIN epoch 1; iteration 372; loss 0.08994065970182419;\n",
      "TRAIN epoch 1; iteration 373; loss 0.1478511542081833;\n",
      "TRAIN epoch 1; iteration 374; loss 0.03318881243467331;\n",
      "TRAIN epoch 1; iteration 375; loss 0.032271090894937515;\n",
      "TRAIN epoch 1; iteration 376; loss 0.11179255694150925;\n",
      "TRAIN epoch 1; iteration 377; loss 0.041168514639139175;\n",
      "TRAIN epoch 1; iteration 378; loss 0.09573650360107422;\n",
      "TRAIN epoch 1; iteration 379; loss 0.10747984051704407;\n",
      "TRAIN epoch 1; iteration 380; loss 0.07420039921998978;\n",
      "TRAIN epoch 1; iteration 381; loss 0.06622809916734695;\n",
      "TRAIN epoch 1; iteration 382; loss 0.06686914712190628;\n",
      "TRAIN epoch 1; iteration 383; loss 0.1920892894268036;\n",
      "TRAIN epoch 1; iteration 384; loss 0.0844881534576416;\n",
      "TRAIN epoch 1; iteration 385; loss 0.058143630623817444;\n",
      "TRAIN epoch 1; iteration 386; loss 0.026591522619128227;\n",
      "TRAIN epoch 1; iteration 387; loss 0.09802184998989105;\n",
      "TRAIN epoch 1; iteration 388; loss 0.23091818392276764;\n",
      "TRAIN epoch 1; iteration 389; loss 0.055419545620679855;\n",
      "TRAIN epoch 1; iteration 390; loss 0.05612188205122948;\n",
      "TRAIN epoch 1; iteration 391; loss 0.023380085825920105;\n",
      "TRAIN epoch 1; iteration 392; loss 0.021326586604118347;\n",
      "TRAIN epoch 1; iteration 393; loss 0.09292015433311462;\n",
      "TRAIN epoch 1; iteration 394; loss 0.04375050589442253;\n",
      "TRAIN epoch 1; iteration 395; loss 0.12464199215173721;\n",
      "TRAIN epoch 1; iteration 396; loss 0.10493786633014679;\n",
      "TRAIN epoch 1; iteration 397; loss 0.0823265090584755;\n",
      "TRAIN epoch 1; iteration 398; loss 0.026182441040873528;\n",
      "TRAIN epoch 1; iteration 399; loss 0.047385044395923615;\n",
      "TRAIN epoch 1; iteration 400; loss 0.01182860042899847;\n",
      "saving optimizer state to other.padl/0.pt\n",
      "saving iterate args to other.padl/0.iterate.json\n",
      "saving metrics to other.padl/0.metrics.json\n",
      "saving iteration to other.padl/0.iteration.json\n",
      "saving epoch to other.padl/0.epoch.json\n",
      "saving torch module to other.padl/10.pt\n",
      "TRAIN epoch 1; iteration 401; loss 0.05931166186928749;\n",
      "TRAIN epoch 1; iteration 402; loss 0.019649861380457878;\n",
      "TRAIN epoch 1; iteration 403; loss 0.17845897376537323;\n",
      "TRAIN epoch 1; iteration 404; loss 0.06017010286450386;\n",
      "TRAIN epoch 1; iteration 405; loss 0.007129449862986803;\n",
      "TRAIN epoch 1; iteration 406; loss 0.024341726675629616;\n",
      "TRAIN epoch 1; iteration 407; loss 0.023532439023256302;\n",
      "TRAIN epoch 1; iteration 408; loss 0.049725111573934555;\n",
      "TRAIN epoch 1; iteration 409; loss 0.07535167038440704;\n",
      "TRAIN epoch 1; iteration 410; loss 0.13837775588035583;\n",
      "TRAIN epoch 1; iteration 411; loss 0.04129137098789215;\n",
      "TRAIN epoch 1; iteration 412; loss 0.02813347987830639;\n",
      "TRAIN epoch 1; iteration 413; loss 0.06631193310022354;\n",
      "TRAIN epoch 1; iteration 414; loss 0.04350869730114937;\n",
      "TRAIN epoch 1; iteration 415; loss 0.018901044502854347;\n",
      "TRAIN epoch 1; iteration 416; loss 0.05749798193573952;\n",
      "TRAIN epoch 1; iteration 417; loss 0.10912200808525085;\n",
      "TRAIN epoch 1; iteration 418; loss 0.1327153742313385;\n",
      "TRAIN epoch 1; iteration 419; loss 0.04607976973056793;\n",
      "TRAIN epoch 1; iteration 420; loss 0.04876707121729851;\n",
      "TRAIN epoch 1; iteration 421; loss 0.04147294908761978;\n",
      "TRAIN epoch 1; iteration 422; loss 0.037677183747291565;\n",
      "TRAIN epoch 1; iteration 423; loss 0.056401729583740234;\n",
      "TRAIN epoch 1; iteration 424; loss 0.0783986747264862;\n",
      "TRAIN epoch 1; iteration 425; loss 0.07520155608654022;\n",
      "TRAIN epoch 1; iteration 426; loss 0.07991138100624084;\n",
      "TRAIN epoch 1; iteration 427; loss 0.10332319885492325;\n",
      "TRAIN epoch 1; iteration 428; loss 0.018193822354078293;\n",
      "TRAIN epoch 1; iteration 429; loss 0.0469578318297863;\n",
      "TRAIN epoch 1; iteration 430; loss 0.06926646828651428;\n",
      "TRAIN epoch 1; iteration 431; loss 0.20259366929531097;\n",
      "TRAIN epoch 1; iteration 432; loss 0.0496327206492424;\n",
      "TRAIN epoch 1; iteration 433; loss 0.005859511438757181;\n",
      "TRAIN epoch 1; iteration 434; loss 0.02203291468322277;\n",
      "TRAIN epoch 1; iteration 435; loss 0.060563549399375916;\n",
      "TRAIN epoch 1; iteration 436; loss 0.0699026957154274;\n",
      "TRAIN epoch 1; iteration 437; loss 0.057597171515226364;\n",
      "TRAIN epoch 1; iteration 438; loss 0.06799113005399704;\n",
      "TRAIN epoch 1; iteration 439; loss 0.058978211134672165;\n",
      "TRAIN epoch 1; iteration 440; loss 0.0341392382979393;\n",
      "TRAIN epoch 1; iteration 441; loss 0.0741347149014473;\n",
      "TRAIN epoch 1; iteration 442; loss 0.059971246868371964;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 1; iteration 443; loss 0.09743323922157288;\n",
      "TRAIN epoch 1; iteration 444; loss 0.14148859679698944;\n",
      "TRAIN epoch 1; iteration 445; loss 0.02676842175424099;\n",
      "TRAIN epoch 1; iteration 446; loss 0.034140702337026596;\n",
      "TRAIN epoch 1; iteration 447; loss 0.04888555407524109;\n",
      "TRAIN epoch 1; iteration 448; loss 0.19648009538650513;\n",
      "TRAIN epoch 1; iteration 449; loss 0.06842663884162903;\n",
      "TRAIN epoch 1; iteration 450; loss 0.005826601758599281;\n",
      "TRAIN epoch 1; iteration 451; loss 0.03234782814979553;\n",
      "TRAIN epoch 1; iteration 452; loss 0.07890503108501434;\n",
      "TRAIN epoch 1; iteration 453; loss 0.08036923408508301;\n",
      "TRAIN epoch 1; iteration 454; loss 0.042893338948488235;\n",
      "TRAIN epoch 1; iteration 455; loss 0.0717347264289856;\n",
      "TRAIN epoch 1; iteration 456; loss 0.013118157163262367;\n",
      "TRAIN epoch 1; iteration 457; loss 0.020496578887104988;\n",
      "TRAIN epoch 1; iteration 458; loss 0.08978584408760071;\n",
      "TRAIN epoch 1; iteration 459; loss 0.07612697035074234;\n",
      "quitting training\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s.train(train_data, 'other.padl', save_interval=100)\n",
    "except KeyboardInterrupt:\n",
    "    print('quitting training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1685e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading optimizer from state-dict\n",
      "loading iterate args\n",
      "loading metrics\n",
      "loading iteration\n",
      "loading epoch\n",
      "loading torch module from other.padl/10.pt\n"
     ]
    }
   ],
   "source": [
    "r = padl.load('other.padl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model we are going to use is the `infer_model`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7e80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mCompose\u001B[0m - \"infer_model\":\n",
       "\n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m0: \u001B[0mlambda x: np.array(x).astype(np.float32)                         \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m1: \u001B[0mlambda x: torch.from_numpy(x).type(torch.float)                  \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m2: \u001B[0mreshape(-1, 28, 28)                                              \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m3: \u001B[0mBatchify(dim=0)                                                  \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m4: \u001B[0mSimpleNet()                                                      \n",
       "   \u001B[32m   │\n",
       "      ▼ args\u001B[0m\n",
       "   \u001B[1m5: \u001B[0mUnbatchify(dim=0, cpu=True)                                      \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m6: \u001B[0mlambda x: x.exp() / x.exp().sum()                                \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m7: \u001B[0mlambda x: x.topk(1)                                              \n",
       "   \u001B[32m   │\n",
       "      ▼ x\u001B[0m\n",
       "   \u001B[1m8: \u001B[0mlambda x: {'probability': x[0].item(), 'prediction': x[1].item()}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.infer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test a few sample predictions to check that everything worked as expected:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e7c1534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoklEQVR4nGNgGDaAEbuwhJ7djxYs4hyqE7Z+//luvQumlErNvX8fTnc5YMqINtz49f/fbCOsFl359+97iTwTNjm1+/8+9rPjcHjJv5sGqCIsSOxbvrdRJZEsYBRRxmEmAwOD6o43rrhlOe7sw2ksw49jilglgxkYGBgYlO9iNfH3qXgN2ZV/ArFKmm7/9ePbz2oWVFF4lGlYqs56gNuxgwMAAMabMMIdrGaYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166CA2A60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9964532256126404, 'prediction': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzUlEQVR4nGNgGHAQ+C8AiceEKqnK0KWMU+fOf/9m49Qph8JjRJW8/krvhyQunYybD4gjeCwocjKCt0Rxurbx5SkGBlfsOl0Tw14gOwpZp/zqV0dQDEKS9LjBl/KCgYHBhgENCIsELv70b5MIq4j7P1TdDOyTfvyDgEP//v275gn3GAMDAwPXLquHe24xMDAwXnpZ5//98ak4ZK1CVnwwpsa/I6wa6LYiSWL1CgMDAwPD398szLg0MjAc+aeOUyfDMsYo3JI//z/CbSztAQBsMz27HwGZFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166C097F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9761355519294739, 'prediction': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nGNgGGSAEYXHLmhiL8L1kOHQ3u/oCvUrdv358/f+uXM//q7lZWBgYGCByQgLd/j/Zzi+//CV5wz2bAxsKNrULv853a3Lid1ytz8nuNHF4MYyMELcJiGqaXJm61dUVZKn/nQwMKhN//vnz58/x7XQvOKyiTFStJvn94EH6xh+HUS3Yc/fF3/+XIrD7iTuv3/+lPAgizAhmCuYGBkZMUIGAmy+vC/88CcBu2TNn0kMQR/uKmCVPP9HlIFh6Z9LOCQZGBg4N/9uxyZ57C8DAwOD6Ze/rlgkld6JMTAwMHT9qYQLIbxy78ehfGEGBQWGM9jMjfv75/b5J39f62B1Uti8P3/+NAhhlaMmAADeEF/OxXfhtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166C95670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9995518326759338, 'prediction': 8}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABI0lEQVR4nL2SsUvDYBDF3xcsKlVU0IKdxEEEcRHB4uIkiGtR0MHNUVBwcXPQpYi4dReEInWICHUQ2kVw6SRkEQf/ADUgKsJ7iUMSYpLi2Fu+4353x33vDui+mdgt/EyXYUa3sHGZzpqqfj6RFMnHNCu7FENYi4JW+G4PAADqDoCXNAQAp1jcbPxtF0FXxiqsvXt5Y8xpZuxjirR3RNb7MnD8IBpotsOPUbL/gcitkyJbS53gUEXyJemwP1t4TfJizyV1NZiGTZLPE5i7o7icRJNnIjkDYKQlnifhPUXWLADYFxNC4Ujy1R4DcguVpq+gbU8I5314/usJzPAq4H1/JCpvGa+M7VIQjIR34ryb3ZWHwIvOpHfRzvtA9avhvDGrT/fsFwvzlbxCH2aWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166F9CC70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9942909479141235, 'prediction': 8}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAoUlEQVR4nMWSwRHCMAwEdzKUgagjog9ciPtIISh9mNRhFyIeyYATxBfuqZWtk8/wb0ku7hozrW65ppCZTwJTDN0ARF6FUwfnM0B7F4bdTGGnHi4cjPbXjkDi0YiU3ar3e+5mcluul3aX4ykA8wyo5xgqgNUIim49kdt29DnwqRZAW19cxvmzP7lsI4NYkiugtmZzlLlZ9RLHiZaSv/yR3+kJi1I5hGMYlT0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166D60A30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9995865821838379, 'prediction': 5}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nNWQwQ3DIAxFzR7JHIU5AlXn6B7dI2QOqDoGitoprI97iEI4BE691L7xePgboh/WNWYsLQgG65YnyLbjheEUjVEgoeHFvrfOHa+R08Xc9jr7jVGQezntec4Hyn7GuZN5diAy2iewn3013AtWTcbFjL0LMwmsaUngows87u9dPVsbYHCYtnNFRER3e1GiRD0/ouRWpA3SsMHXu/EL/1Bf2naTG9CAjuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166FDBEE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9929101467132568, 'prediction': 6}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/klEQVR4nGNgGGSAEcEU26tzex0DAwPDo0Vf0JWxXPoLA9tYoUIIM34zMHy4zcBkzMDgLncXXSubo6MuAwPP379/53Fid4PGjL9/T3DhcOCiv39fmmGVUah98OfvKyNsUszJd/7+/ZyLTSp8wYq/f/+uM8Vq5MO/f//+/RvNhlVy75/bf//+/VuEVVJQQ1xz4t+/r6dglWVgYGCzfv33uz4uWQbDT3/vs+KUXfT3bwScw4QmeR6Zw8TAwMDAZSIP5ZpHM/x8BZdkYWBgYCgv/PCcYcJvBukoHY4/C/ahmhW77uFrWERPRhKHJBMWDh1/BieTp0sZpj79j9Ox1AIAkJlkR4F+UyAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166E12550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999858736991882, 'prediction': 3}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nGNgGDaAEcHU4AgSZWBgCHM9h67IKfXY138QEAwXZGFgYGBgsFvPz/yf8eWnWf+8HZHMgkiKvfx15Nmm688YGEQd/2NYzc0PZbz498cbpwM//HuF4DDh8wqm5G6ckkwMDHdwGpT/758TLjnBm/8eCOCS1Pr3rxSngwIYGK7glFRl+HATl6nyf/7txyXHkPLvnwsyH8VYGQaG07g0Mp/9d40Tl6Tpv3/LUARQXft+Ek73YAAAktA5d9A6x/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166B272B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9992590546607971, 'prediction': 7}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCElEQVR4nNWRrUsDcRzGH6cnsnhhvk3BOOyC130pJssEwbYiCCbBwU4E2YEIgiCyIPgf2JYOg3UsuWBbULdyMNlMdx9mUPHH7leMPvXD8/A836/0D5XZHw5bQbDuuo7kFlvtBQO6/Oi+VouBau4XTt4xohtJY990rjEtJZuS1o6k/mH77dkIXu1Asivlr4GD0U65jXeis9kmUM6mK2d7JB8QVxzbIK8PDE7sa2eegFtjvQnrBbtJUt6PAborNrgDg/NCg3rGAi+gLBXBUvY0wR+XtuE4DUMeHDneq+U+UsiV5gMwYicMvBXtLUq6JGVc7n29KvQtfZZeAKLKlGWHVOryWPWs6O/6BOK1jHxaiUrrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166E450D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9995653033256531, 'prediction': 8}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABGElEQVR4nM3Nvy8DcRjH8c8hkYscEWnDxsB2UyN+LN26iMFgVZXYxEIkVhv/gBmLsthIEElJDCLECaH/wA0dOpw2l3yefA1t7567rgbP9H3n9XzzAP93HPdMxEi51NttxXeSfPkgy102VxPyfg/OObmdtoBCugCcXYYLCcs+s4Owffka17gpIibcasV0IFcaj0iGp5064IOdxOV8VDPkYmx2hRd9cQ5VWYm/jpKzqStZAK39vIWGRsuCFWHOBE2NxsAA6GnnY1VZ/5jeLLE+qXKH/B6MyvG4pvCO3FB5KDIRxYCRz4zC4Rt6xfZ76Ul4rI8id9lonhQKU3DX65TrESRnxSPpv9ZI/swjPc7qrU8h3/YzXfan8wsEyIQ/S6K51QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x166CA2BB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999974966049194, 'prediction': 0}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "for _ in range(10):\n",
    "    datapoint = random.choice(metric_data)\n",
    "    display(datapoint)\n",
    "    print(r.infer_model.infer_apply(datapoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So there you have it, a highly portable and flexible trainer object for pipelines written\n",
    "in PADL and PyTorch. Go forth and train a bazillion models (using highly sustainable energy sources)!\n",
    "\n",
    "**Happy PADL-ling!**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}