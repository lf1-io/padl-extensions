{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install padl-extensions[trainer]\n",
    "!pip install padl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b65f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install huggingface transformers dependencies (pip install padl-extensions[huggingface]) to use connector\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import padl\n",
    "from padl_ext.trainer.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fed61d",
   "metadata": {},
   "source": [
    "This tutorial and accompanying notebook show you how to implement a highly portable training object for PyTorch modules using PADL. We'll be using the classic MNIST dataset and a standard CNN for illustrative purposes. The same approach applies to arbitrary PyTorch models. For more background on PADL see here and here, \n",
    "and a fully working example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ade97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "valid_data = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22499edd",
   "metadata": {},
   "source": [
    "Here's our layer transform implementing the CNN. Notice the decoration `@padl.transform` - all that's necessary to access\n",
    "the full range of cool PADL functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3ef264",
   "metadata": {},
   "outputs": [],
   "source": [
    "@padl.transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89baca81",
   "metadata": {},
   "source": [
    "All tensors in PADL are accessed by pushing data through \"pipelines\" or \"transforms\". \"Transforms\" are the basic\n",
    "building blocks and pipelines are compositions, and branches built up from \"transforms\". \n",
    "\n",
    "In the example below, `train_model` is built up of a preprocessor, which prepares tensors, and additionally\n",
    "a relatively trivial branch for the target labels. The prepared tensors are pushed through the layer, followed by the loss\n",
    "together with the labels.\n",
    "\n",
    "The pipeline makes use of the overloaded operators `>>` (compose) and `/` (apply-in-parallel). For more \n",
    "introduction to these operators see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eee21f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"train_model\":\n",
       "\n",
       "   \u001b[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ args                 ▼ args\u001b[0m\n",
       "   \u001b[1m0: \u001b[0m\u001b[32m[\u001b[0mpreprocess: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mBatchify(dim=0)     \n",
       "   \u001b[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ x                    ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mSimpleNet()          \u001b[32m/\u001b[0m type(torch.int64)\n",
       "   \u001b[32m   │\n",
       "      ▼ (input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mcross_entropy       "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = (\n",
    "    padl.transform(lambda x: np.array(x).astype(np.float32))\n",
    "    >> padl.transform(lambda x: torch.from_numpy(x).type(torch.float))\n",
    "    >> padl.same.reshape(-1, 28, 28)\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess / padl.identity\n",
    "    >> padl.batch\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    "    >> padl.transform(F.cross_entropy)\n",
    ")\n",
    "\n",
    "train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3299dfc",
   "metadata": {},
   "source": [
    "When we eventually use the trained layer, we won't need the loss or the labels. For that reason we create an\n",
    "additional pipeline, whose weights are tied to `train_model`, which we'll use in testing, demo-ing, serving etc..\n",
    "\n",
    "This model may contain non-PyTorch postprocessing (everything after the `unbatch`) which can come in handy\n",
    "when communicating with other bits of your infrastructure, such as returning results in the body of a response etc.. In this case, we add the results to a dictionary, along with the confidence estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26479a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"infer_model\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mlambda x: np.array(x).astype(np.float32)                         \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlambda x: torch.from_numpy(x).type(torch.float)                  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mreshape(-1, 28, 28)                                              \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mBatchify(dim=0)                                                  \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()                                                      \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mUnbatchify(dim=0, cpu=True)                                      \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m6: \u001b[0mlambda x: x.exp() / x.exp().sum()                                \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m7: \u001b[0mlambda x: x.topk(1)                                              \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m8: \u001b[0mlambda x: {'probability': x[0].item(), 'prediction': x[1].item()}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_model = (\n",
    "    preprocess\n",
    "    >> padl.batch\n",
    "    >> simplenet\n",
    "    >> padl.unbatch\n",
    "    >> padl.transform(lambda x: x.exp() / x.exp().sum())\n",
    "    >> padl.transform(lambda x: x.topk(1))\n",
    "    >> padl.transform(lambda x: {'probability': x[0].item(), 'prediction': x[1].item()})\n",
    ")\n",
    "infer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4dd50",
   "metadata": {},
   "source": [
    "In order to monitor performance, let's create a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d166d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y):\n",
    "    return sum([xx['prediction'] == yy for xx, yy in zip(x, y)]) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc93226",
   "metadata": {},
   "source": [
    "The torch-extensions package contains a simple trainer, which may be configured to cover many use cases.\n",
    "In order to extend the trainer, the methods may be simply overwritten. Alternatively, simply create a new \n",
    "transform object, with methods to manage training, saving etc.. The `@padl.transform` decorator along with\n",
    "the methods `Transform.pre_load` and `Transform.post_load` will handle any important side effects which you\n",
    "need in order to save the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "736a2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Trainer(\n",
    "    train_model=train_model,\n",
    "    infer_model=infer_model,\n",
    "    optimizer=torch.optim.Adam(train_model.pd_parameters()),\n",
    "    metrics={'accuracy': accuracy}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97f45454",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 0; iteration 0; loss 0.06647056341171265;\n",
      "TRAIN epoch 0; iteration 0;loss: 0.09319868170772679;accuracy: 0.9689;\n",
      "saving optimizer state to train.padl/0.pt\n",
      "saving iterate args to train.padl/0.iterate.json\n",
      "saving metrics to train.padl/0.metrics.json\n",
      "saving iteration to train.padl/0.iteration.json\n",
      "saving epoch to train.padl/0.epoch.json\n",
      "saving torch module to train.padl/10.pt\n",
      "TRAIN epoch 0; iteration 1; loss 0.06425859779119492;\n",
      "TRAIN epoch 0; iteration 2; loss 0.06514322012662888;\n",
      "TRAIN epoch 0; iteration 3; loss 0.018533768132328987;\n",
      "TRAIN epoch 0; iteration 4; loss 0.1741870939731598;\n",
      "TRAIN epoch 0; iteration 5; loss 0.15765243768692017;\n",
      "TRAIN epoch 0; iteration 6; loss 0.03293801099061966;\n",
      "TRAIN epoch 0; iteration 7; loss 0.07093793898820877;\n",
      "TRAIN epoch 0; iteration 8; loss 0.052767299115657806;\n",
      "TRAIN epoch 0; iteration 9; loss 0.11194372177124023;\n",
      "TRAIN epoch 0; iteration 10; loss 0.08608174324035645;\n",
      "TRAIN epoch 0; iteration 11; loss 0.06020808592438698;\n",
      "TRAIN epoch 0; iteration 12; loss 0.09986421465873718;\n",
      "TRAIN epoch 0; iteration 13; loss 0.07809600234031677;\n",
      "TRAIN epoch 0; iteration 14; loss 0.057585038244724274;\n",
      "TRAIN epoch 0; iteration 15; loss 0.09410332888364792;\n",
      "TRAIN epoch 0; iteration 16; loss 0.09185567498207092;\n",
      "TRAIN epoch 0; iteration 17; loss 0.05325460061430931;\n",
      "TRAIN epoch 0; iteration 18; loss 0.03527548909187317;\n",
      "TRAIN epoch 0; iteration 19; loss 0.174001544713974;\n",
      "TRAIN epoch 0; iteration 20; loss 0.09515455365180969;\n",
      "TRAIN epoch 0; iteration 21; loss 0.15121568739414215;\n",
      "TRAIN epoch 0; iteration 22; loss 0.031880978494882584;\n",
      "TRAIN epoch 0; iteration 23; loss 0.027240276336669922;\n",
      "TRAIN epoch 0; iteration 24; loss 0.10312966257333755;\n",
      "TRAIN epoch 0; iteration 25; loss 0.05588053539395332;\n",
      "TRAIN epoch 0; iteration 26; loss 0.1869024783372879;\n",
      "TRAIN epoch 0; iteration 27; loss 0.10837855190038681;\n",
      "TRAIN epoch 0; iteration 28; loss 0.07864074409008026;\n",
      "TRAIN epoch 0; iteration 29; loss 0.022346030920743942;\n",
      "TRAIN epoch 0; iteration 30; loss 0.044293422251939774;\n",
      "TRAIN epoch 0; iteration 31; loss 0.018026180565357208;\n",
      "TRAIN epoch 0; iteration 32; loss 0.07068858295679092;\n",
      "TRAIN epoch 0; iteration 33; loss 0.0294210035353899;\n",
      "TRAIN epoch 0; iteration 34; loss 0.043963026255369186;\n",
      "TRAIN epoch 0; iteration 35; loss 0.11901587247848511;\n",
      "TRAIN epoch 0; iteration 36; loss 0.21677441895008087;\n",
      "TRAIN epoch 0; iteration 37; loss 0.10712435096502304;\n",
      "TRAIN epoch 0; iteration 38; loss 0.09997700899839401;\n",
      "TRAIN epoch 0; iteration 39; loss 0.0499567948281765;\n",
      "TRAIN epoch 0; iteration 40; loss 0.1722342073917389;\n",
      "TRAIN epoch 0; iteration 41; loss 0.146629199385643;\n",
      "TRAIN epoch 0; iteration 42; loss 0.10180312395095825;\n",
      "TRAIN epoch 0; iteration 43; loss 0.07162854075431824;\n",
      "TRAIN epoch 0; iteration 44; loss 0.16366304457187653;\n",
      "TRAIN epoch 0; iteration 45; loss 0.1035098060965538;\n",
      "TRAIN epoch 0; iteration 46; loss 0.05948582664132118;\n",
      "TRAIN epoch 0; iteration 47; loss 0.056917767971754074;\n",
      "TRAIN epoch 0; iteration 48; loss 0.023909039795398712;\n",
      "TRAIN epoch 0; iteration 49; loss 0.10119408369064331;\n",
      "TRAIN epoch 0; iteration 50; loss 0.0652126744389534;\n",
      "TRAIN epoch 0; iteration 51; loss 0.07568176835775375;\n",
      "TRAIN epoch 0; iteration 52; loss 0.10442890971899033;\n",
      "TRAIN epoch 0; iteration 53; loss 0.06765959411859512;\n",
      "TRAIN epoch 0; iteration 54; loss 0.021725444123148918;\n",
      "TRAIN epoch 0; iteration 55; loss 0.08652158826589584;\n",
      "TRAIN epoch 0; iteration 56; loss 0.11137863248586655;\n",
      "TRAIN epoch 0; iteration 57; loss 0.15472078323364258;\n",
      "TRAIN epoch 0; iteration 58; loss 0.157415971159935;\n",
      "TRAIN epoch 0; iteration 59; loss 0.056641608476638794;\n",
      "TRAIN epoch 0; iteration 60; loss 0.06162884086370468;\n",
      "TRAIN epoch 0; iteration 61; loss 0.0743860974907875;\n",
      "TRAIN epoch 0; iteration 62; loss 0.09328213334083557;\n",
      "TRAIN epoch 0; iteration 63; loss 0.09440365433692932;\n",
      "TRAIN epoch 0; iteration 64; loss 0.06765599548816681;\n",
      "TRAIN epoch 0; iteration 65; loss 0.07672462612390518;\n",
      "TRAIN epoch 0; iteration 66; loss 0.0799371749162674;\n",
      "TRAIN epoch 0; iteration 67; loss 0.11381799727678299;\n",
      "TRAIN epoch 0; iteration 68; loss 0.3200681805610657;\n",
      "TRAIN epoch 0; iteration 69; loss 0.11673814058303833;\n",
      "TRAIN epoch 0; iteration 70; loss 0.20556670427322388;\n",
      "TRAIN epoch 0; iteration 71; loss 0.11692903190851212;\n",
      "TRAIN epoch 0; iteration 72; loss 0.2336064577102661;\n",
      "TRAIN epoch 0; iteration 73; loss 0.11194746941328049;\n",
      "TRAIN epoch 0; iteration 74; loss 0.04080982506275177;\n",
      "quitting training...\n"
     ]
    }
   ],
   "source": [
    "metric_data = [x[0] for x in valid_data]\n",
    "ground_truth = [x[1] for x in valid_data]\n",
    "\n",
    "try:\n",
    "    t.train(train_data, 'train.padl', valid_data=valid_data,\n",
    "            save_interval=100, batch_size=100, metric_data=metric_data, ground_truth=ground_truth)\n",
    "except KeyboardInterrupt:\n",
    "    print('quitting training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6941ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading optimizer from state-dict\n",
      "loading iterate args\n",
      "loading metrics\n",
      "loading iteration\n",
      "loading epoch\n",
      "loading torch module from train.padl/10.pt\n"
     ]
    }
   ],
   "source": [
    "from padl import load\n",
    "\n",
    "s = load('train.padl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efa1602c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN epoch 1; iteration 1; loss 0.01650511473417282;\n",
      "TRAIN epoch 1; iteration 2; loss 0.0685797855257988;\n",
      "TRAIN epoch 1; iteration 3; loss 0.06085962802171707;\n",
      "TRAIN epoch 1; iteration 4; loss 0.03802616521716118;\n",
      "TRAIN epoch 1; iteration 5; loss 0.12736043334007263;\n",
      "TRAIN epoch 1; iteration 6; loss 0.1305537074804306;\n",
      "TRAIN epoch 1; iteration 7; loss 0.04711392521858215;\n",
      "TRAIN epoch 1; iteration 8; loss 0.05891698971390724;\n",
      "TRAIN epoch 1; iteration 9; loss 0.08455874770879745;\n",
      "TRAIN epoch 1; iteration 10; loss 0.1347479671239853;\n",
      "TRAIN epoch 1; iteration 11; loss 0.09008970856666565;\n",
      "TRAIN epoch 1; iteration 12; loss 0.0694253146648407;\n",
      "TRAIN epoch 1; iteration 13; loss 0.07315275073051453;\n",
      "TRAIN epoch 1; iteration 14; loss 0.08259107917547226;\n",
      "TRAIN epoch 1; iteration 15; loss 0.033659759908914566;\n",
      "TRAIN epoch 1; iteration 16; loss 0.10053250938653946;\n",
      "TRAIN epoch 1; iteration 17; loss 0.09968714416027069;\n",
      "TRAIN epoch 1; iteration 18; loss 0.07941127568483353;\n",
      "TRAIN epoch 1; iteration 19; loss 0.03296836465597153;\n",
      "TRAIN epoch 1; iteration 20; loss 0.11491996049880981;\n",
      "TRAIN epoch 1; iteration 21; loss 0.12114565074443817;\n",
      "TRAIN epoch 1; iteration 22; loss 0.07275308668613434;\n",
      "TRAIN epoch 1; iteration 23; loss 0.02977893315255642;\n",
      "TRAIN epoch 1; iteration 24; loss 0.03544330224394798;\n",
      "TRAIN epoch 1; iteration 25; loss 0.10567030310630798;\n",
      "TRAIN epoch 1; iteration 26; loss 0.09098242968320847;\n",
      "TRAIN epoch 1; iteration 27; loss 0.13311667740345;\n",
      "TRAIN epoch 1; iteration 28; loss 0.1126914769411087;\n",
      "TRAIN epoch 1; iteration 29; loss 0.06718368828296661;\n",
      "TRAIN epoch 1; iteration 30; loss 0.06127314642071724;\n",
      "TRAIN epoch 1; iteration 31; loss 0.04484819993376732;\n",
      "TRAIN epoch 1; iteration 32; loss 0.0134775685146451;\n",
      "TRAIN epoch 1; iteration 33; loss 0.1135120689868927;\n",
      "TRAIN epoch 1; iteration 34; loss 0.05032925680279732;\n",
      "TRAIN epoch 1; iteration 35; loss 0.038228534162044525;\n",
      "TRAIN epoch 1; iteration 36; loss 0.10002870857715607;\n",
      "TRAIN epoch 1; iteration 37; loss 0.2568666934967041;\n",
      "TRAIN epoch 1; iteration 38; loss 0.09870216250419617;\n",
      "TRAIN epoch 1; iteration 39; loss 0.08033514022827148;\n",
      "TRAIN epoch 1; iteration 40; loss 0.09298992902040482;\n",
      "TRAIN epoch 1; iteration 41; loss 0.14913679659366608;\n",
      "TRAIN epoch 1; iteration 42; loss 0.13826806843280792;\n",
      "TRAIN epoch 1; iteration 43; loss 0.09999945014715195;\n",
      "TRAIN epoch 1; iteration 44; loss 0.09081105142831802;\n",
      "TRAIN epoch 1; iteration 45; loss 0.13887812197208405;\n",
      "TRAIN epoch 1; iteration 46; loss 0.07755479216575623;\n",
      "TRAIN epoch 1; iteration 47; loss 0.05539042502641678;\n",
      "TRAIN epoch 1; iteration 48; loss 0.056893881410360336;\n",
      "TRAIN epoch 1; iteration 49; loss 0.130384162068367;\n",
      "TRAIN epoch 1; iteration 50; loss 0.08228009194135666;\n",
      "TRAIN epoch 1; iteration 51; loss 0.07716356217861176;\n",
      "TRAIN epoch 1; iteration 52; loss 0.0875452533364296;\n",
      "TRAIN epoch 1; iteration 53; loss 0.1138807013630867;\n",
      "TRAIN epoch 1; iteration 54; loss 0.09117438644170761;\n",
      "TRAIN epoch 1; iteration 55; loss 0.08319143205881119;\n",
      "TRAIN epoch 1; iteration 56; loss 0.11057057231664658;\n",
      "TRAIN epoch 1; iteration 57; loss 0.17637386918067932;\n",
      "TRAIN epoch 1; iteration 58; loss 0.20625834167003632;\n",
      "TRAIN epoch 1; iteration 59; loss 0.16405171155929565;\n",
      "TRAIN epoch 1; iteration 60; loss 0.06971463561058044;\n",
      "TRAIN epoch 1; iteration 61; loss 0.07095617055892944;\n",
      "TRAIN epoch 1; iteration 62; loss 0.08779281377792358;\n",
      "TRAIN epoch 1; iteration 63; loss 0.05261552706360817;\n",
      "TRAIN epoch 1; iteration 64; loss 0.045640863478183746;\n",
      "TRAIN epoch 1; iteration 65; loss 0.17179685831069946;\n",
      "TRAIN epoch 1; iteration 66; loss 0.03296447917819023;\n",
      "TRAIN epoch 1; iteration 67; loss 0.06947598606348038;\n",
      "TRAIN epoch 1; iteration 68; loss 0.03385333716869354;\n",
      "TRAIN epoch 1; iteration 69; loss 0.2367643415927887;\n",
      "TRAIN epoch 1; iteration 70; loss 0.1345880627632141;\n",
      "TRAIN epoch 1; iteration 71; loss 0.1771484762430191;\n",
      "TRAIN epoch 1; iteration 72; loss 0.08848200738430023;\n",
      "TRAIN epoch 1; iteration 73; loss 0.19840514659881592;\n",
      "TRAIN epoch 1; iteration 74; loss 0.07540702074766159;\n",
      "TRAIN epoch 1; iteration 75; loss 0.05335601791739464;\n",
      "TRAIN epoch 1; iteration 76; loss 0.1085304319858551;\n",
      "TRAIN epoch 1; iteration 77; loss 0.09555432945489883;\n",
      "TRAIN epoch 1; iteration 78; loss 0.13600784540176392;\n",
      "TRAIN epoch 1; iteration 79; loss 0.09560799598693848;\n",
      "TRAIN epoch 1; iteration 80; loss 0.08994831144809723;\n",
      "TRAIN epoch 1; iteration 81; loss 0.01940862089395523;\n",
      "TRAIN epoch 1; iteration 82; loss 0.05840266868472099;\n",
      "TRAIN epoch 1; iteration 83; loss 0.2001032680273056;\n",
      "TRAIN epoch 1; iteration 84; loss 0.0746430978178978;\n",
      "TRAIN epoch 1; iteration 85; loss 0.09358256310224533;\n",
      "TRAIN epoch 1; iteration 86; loss 0.02172505296766758;\n",
      "TRAIN epoch 1; iteration 87; loss 0.09436209499835968;\n",
      "TRAIN epoch 1; iteration 88; loss 0.22162213921546936;\n",
      "TRAIN epoch 1; iteration 89; loss 0.10623311251401901;\n",
      "TRAIN epoch 1; iteration 90; loss 0.0984741747379303;\n",
      "TRAIN epoch 1; iteration 91; loss 0.05324586480855942;\n",
      "TRAIN epoch 1; iteration 92; loss 0.06383206695318222;\n",
      "TRAIN epoch 1; iteration 93; loss 0.14401914179325104;\n",
      "TRAIN epoch 1; iteration 94; loss 0.11114969104528427;\n",
      "TRAIN epoch 1; iteration 95; loss 0.2463921159505844;\n",
      "TRAIN epoch 1; iteration 96; loss 0.05313321202993393;\n",
      "TRAIN epoch 1; iteration 97; loss 0.03777264058589935;\n",
      "TRAIN epoch 1; iteration 98; loss 0.0458320677280426;\n",
      "TRAIN epoch 1; iteration 99; loss 0.05191708728671074;\n",
      "TRAIN epoch 1; iteration 100; loss 0.015516603365540504;\n",
      "saving optimizer state to other.padl/0.pt\n",
      "saving iterate args to other.padl/0.iterate.json\n",
      "saving metrics to other.padl/0.metrics.json\n",
      "saving iteration to other.padl/0.iteration.json\n",
      "saving epoch to other.padl/0.epoch.json\n",
      "saving torch module to other.padl/10.pt\n",
      "TRAIN epoch 1; iteration 101; loss 0.12297511100769043;\n",
      "TRAIN epoch 1; iteration 102; loss 0.04665730148553848;\n",
      "TRAIN epoch 1; iteration 103; loss 0.17475871741771698;\n",
      "TRAIN epoch 1; iteration 104; loss 0.08614939451217651;\n",
      "TRAIN epoch 1; iteration 105; loss 0.019633114337921143;\n",
      "TRAIN epoch 1; iteration 106; loss 0.045218974351882935;\n",
      "TRAIN epoch 1; iteration 107; loss 0.02550250105559826;\n",
      "TRAIN epoch 1; iteration 108; loss 0.08920461684465408;\n",
      "TRAIN epoch 1; iteration 109; loss 0.17798088490962982;\n",
      "TRAIN epoch 1; iteration 110; loss 0.11391594260931015;\n",
      "TRAIN epoch 1; iteration 111; loss 0.07986170053482056;\n",
      "TRAIN epoch 1; iteration 112; loss 0.02418932504951954;\n",
      "TRAIN epoch 1; iteration 113; loss 0.12645088136196136;\n",
      "TRAIN epoch 1; iteration 114; loss 0.08202876150608063;\n",
      "TRAIN epoch 1; iteration 115; loss 0.02325413189828396;\n",
      "TRAIN epoch 1; iteration 116; loss 0.11574727296829224;\n",
      "TRAIN epoch 1; iteration 117; loss 0.2370481938123703;\n",
      "TRAIN epoch 1; iteration 118; loss 0.1744333654642105;\n",
      "TRAIN epoch 1; iteration 119; loss 0.09230798482894897;\n",
      "TRAIN epoch 1; iteration 120; loss 0.012949176132678986;\n",
      "TRAIN epoch 1; iteration 121; loss 0.07361660152673721;\n",
      "TRAIN epoch 1; iteration 122; loss 0.03974783048033714;\n",
      "TRAIN epoch 1; iteration 123; loss 0.06437844038009644;\n",
      "quitting training\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s.train(train_data, 'other.padl', save_interval=100)\n",
    "except KeyboardInterrupt:\n",
    "    print('quitting training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95023ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading optimizer from state-dict\n",
      "loading iterate args\n",
      "loading metrics\n",
      "loading iteration\n",
      "loading epoch\n",
      "loading torch module from other.padl/10.pt\n"
     ]
    }
   ],
   "source": [
    "r = padl.load('other.padl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6446c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"infer_model\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mlambda x: np.array(x).astype(np.float32)                         \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlambda x: torch.from_numpy(x).type(torch.float)                  \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mreshape(-1, 28, 28)                                              \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mBatchify(dim=0)                                                  \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()                                                      \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mUnbatchify(dim=0, cpu=True)                                      \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m6: \u001b[0mlambda x: x.exp() / x.exp().sum()                                \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m7: \u001b[0mlambda x: x.topk(1)                                              \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m8: \u001b[0mlambda x: {'probability': x[0].item(), 'prediction': x[1].item()}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.infer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "008931c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nNXOL0uDYRSG8ct3KG4MRYyKmvwDCwuGDdGvoIggWDUszCZ+BkHUsKZp1WIRxCADVxbF4DA4g0xEEIOwpUsMotu77Q1G73Ie+J3nnAP/NcHO0yYAuc/9gU5MaQmAR00CBC2b3IUmwNJI19DRqhbGAe71rD9sC1qfBhJTNc0H7ZasqrUxSJdU3xLtOKs2toGZunqXDU09Ui+BoVO1OBiy+IN6kckUKupWPHxqzlaufvcFdKacb/w8+75LbH6xDBxkeV2/7uoHYO5dl3sTw+d6nIjADa3EI2z1Qw8jjKJ6O9Hb1prqXsTHE31OxyJw5eYlFbXxz/kC4sZu8Xic6lAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16B0A8820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999269843101501, 'prediction': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4ElEQVR4nM3RMUtCARTF8WubSIOoYYNgbglv6Qu01GJfoSnBVZAc3ASXFhdxb+0bVENTTo6+sSGoQQwHQSmC/+k5GAS++zaHznTgx71cuGb/NK1lJN1WMp4FcwTw5GGuef8wAr7qCZvTDdDroWuZmzmIjme1sSRFUns/ZtkPAYRAGMOzTzHrn2ffgXJMq1fBkZmVRnCdcLFZD6bV3763jc+pVPEkCcO36MeScLr46zE0Mzt2sHx3uil5Z+BR6haDmaJLB5sgQvi+cDD3snnqwLvDeisBk4KLVhjC5MC33WUNtyhqL2E9wCIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16AA1FBB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9897213578224182, 'prediction': 3}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABCElEQVR4nGNgGGSAEcEU5jKyY2BIOX+WIYXR6wiyIuG5qx7++fP3DxRD5FigkjMDGP9jGAuTlIRQz1cwFDEwMHwpR1GkdvXRiUJeBgauzf///v2bitV99kf//P3zJw2rnM2rP3/+vgxgwyaX+vHPnz+bxLBJ6W76++/v3wasRlY8/PPn759VnNjkNkAC4OLRUBlMyYfw0LnogyF58t/fv3+hGKYXFkIM5iEMDAwMDGpelv8ZJJ9gdRUDA8PSP3+6oUwmmFiqMYQ2lkKog8anwlGJr7MZzhlJhjMw/meQRzXWEiku/7yEhRHU2LcnEArX2bxCNZaBq4znv5r311kMc7+8/Y7LrVQCAFBpfsFq70qZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16ADED4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999956488609314, 'prediction': 3}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nMXQv0oDQRAG8G8XiSgIQlr/pEkpJ8EgYhHsBIOPkCaFrS9hF6tUaQQLiXZylWBx4CkIVjamERL1Gos8QOCbW4tdc7cXbXWaXea3szO7wP9HV0R+tQnJ9/2fUUhS+jP5pTKwbHFUL2LUBzoWGRfqInOJVTGDTuPJyImPTUpcbnJwjMVQhlW/8JbC+/PtdWD+Qj57Hu6QEu/ZfTDtqe1yCCCJ7P55WuKwoZRWLnWqlfLQGJN+V4SpqR3kcHMNwF02QmkhP9BD9mlzPeHLSv7aMDt31AbGCQDAta7cVHX61oXaaAE63X30HnpF960k5brkGeofGSZbKEQwdjgZBkUDamckha/tWfrL+ALKXopghTx2QQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16AFD62E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999945759773254, 'prediction': 0}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAt0lEQVR4nGNgGBmAEVOIX8iVl+HFxy0Yks42vgxi0gwMDAwnrBkYWFDkIhczvVn68LcAAwPDPnQDvda8Wq6D3XrZZb9fpeJy24a/fx/4Y5ey/vrn1erKg9+SkMTgDrowac9eBobvdrLYJL9WMjAwMDz9L4kkycTAwMDAwGgE5ZownMGwcNF2CJ318pIghrF/nI9uuynFqx9wrvc9hk7+5O2f/v59dMaXH0UYHraS7AzvP2L6cAgBAGQ0NTdICT8KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16B02FE50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.996069610118866, 'prediction': 5}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyklEQVR4nM2QoQ7CMBCG/yNzk7AENSSZ3fSwswyN5AWQ8AC8DDwEBFwTLNO4JaBIwLQ7xLIW2s0h+E3v7s/X9j/g7yWqrCl7thcMeNpijhIACEPSE09X0b7vAQCjcMk0qBGio0uCL/VhJpr0s5qY0OvpJJjLMgaAgxTuh3K6nhMgT9nhACUfQkkllVy7b97Jj4mIiG7a1ImTBXACVhEPjWtJVGVLlEa86eKwVFVsOu/bHHNRmM66NmjbTyP1sR9bEatZp4ndttv7md7Cvzlz9L4DoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16AD95B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9999507069587708, 'prediction': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBElEQVR4nGNgGGSAkYGBgVONwVeKIYaH8T8DQ2/FX1QFF/5CwJ8Fr//+TUCSYGFgYBD8vP4yA8OSLwzfOD6yiKNJ6shdhfJSmHHbX/Dz71EBHJI5f//+PezvyYFFiiPvJ8Rdt4wwDNc5/Pfv3zP7C3e++fvXFk3O+ePfPz8WczIwMMjt+vuYCU3y74U8KFPw7d8iVEk2Y3Y4O/LvbSGcXmK6/XcKhIFF8t9/BkackggjsIiJcjM8xKm+7u9PbWQB8crVPjC279+/9RAWxGaGgLUM/+5f3rg2jYnBy5b5s/NZFLMSj9yHRvnfnyt4GVB0MjAwiBlViF59rX2VYcFpfD6gAgAAEchmdtKs7moAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16A9F27F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9993990063667297, 'prediction': 3}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7UlEQVR4nGNgGHig/yQIt6T85+fGcA4jkoRS4BvFKAXmzQEYWjhyT/z8+/fvv7+/0jB1Gp9i/M+w4+3x+98OYVqW9+9/MLoYC4zx//+H3eiSTAjm69/oknA71a4zqn5wF2b8f/nIHww7hf/++/v3799/f/9Ox3QQw5z///79KC07+u9fE6Yke26OnCADA0vp3+8eWPRCANv5v1sxXQsFvzYySLHjkmR4y/DsJ7pkjDWUYcV4HMPY/9udORgYGBjCgv7fxzCN9+PfjZGqqj2v/x7ixbTL9smvv3///f27QACrL3JO7rl7MhS7HL0AALdbWhSe8tw2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16AAD59A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9997953772544861, 'prediction': 5}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGGjA0XHnrA0uyfp/N38exyEX/rWXYcYPZaxyUj82iTFk/gvFJsey67McA4MakiQLQlLMJfARAwM7knImBDOCYRcDA0P4n8foRvoXCsu+mMbIwMDQ8gXDvrLvJ32/qDMwMDBcOYbpmoZ//14zMDAwSL6Zhmlnx2uGFwwMDAxOwlgc9GMCw04GBgYG1/8XsLj2MtRrn/Zh9QoDAwOnTxRfoRmmi+x+1TBwX/r379+/H7d0MGR1OBmyPj74lxG64usjTUzNDF8vvdrAycDg/e8oMxbJf//iGRgYmOz+tXJiSL7+d4GPgYGBgbn9ryqGZPxTJyjLnguLrXQBAKk5TyR3CChVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16AB2D970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.721412718296051, 'prediction': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nGNgGAxA99wqEVxyHl/+/avBIdf78Xfm5pXY5VK//I5mKIJJMqHImc5kTl6Kw0zeE/8SGRhYDmE11ubfC2EGBv9/ndgkN/3zZWBgWHSNGYuc4Y/PDAwMPr97sGlc+K+WgYFh5RszLHJh/08yMDDE/YvEIse85YEMA4PdjxOcWCRj/81jYJB/+80Im40bnqgxMNz/F45Nzu7XSgaly/+msGCT7P0XY/vs32UBbHIMvf9O/v43G6sUg+Cbf/8+9mILGgYGhqoft5qdsUsxMKzswCVDZwAAYHNI8Sux8A4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x16B0EF940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.880384624004364, 'prediction': 0}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "for _ in range(10):\n",
    "    datapoint = random.choice(metric_data)\n",
    "    display(datapoint)\n",
    "    print(r.infer_model.infer_apply(datapoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
