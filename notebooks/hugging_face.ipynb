{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2014.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "!unzip train2014.zip\n",
    "!unzip annotations_trainval2014.zip\n",
    "!rm train2014.zip\n",
    "!rm annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 -m pip install padl\n",
    "!python3.9 -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7ccee7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
      "No model was supplied, defaulted to google/vit-base-patch16-224 (https://huggingface.co/google/vit-base-patch16-224)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import padl\n",
    "\n",
    "tg = pipeline('text-generation')\n",
    "pl = pipeline('image-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5fc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open('annotations/captions_train2014.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "image_lookup = {}\n",
    "for image in data['images']:\n",
    "    image_lookup[image['id']] = 'train2014/' + image['file_name']\n",
    "    \n",
    "annotations = []\n",
    "for annotation in data['annotations']:\n",
    "    annotations.append({'image': image_lookup[annotation['image_id']], 'caption': annotation['caption']})\n",
    "    \n",
    "annotations = pandas.DataFrame(annotations)\n",
    "all_images = annotations['image'].unique().tolist()\n",
    "\n",
    "random.shuffle(all_images)\n",
    "\n",
    "train_images = all_images[:-1000]\n",
    "valid_images = all_images[-1000:]\n",
    "\n",
    "train_annotations = annotations[annotations['image'].isin(train_images)].to_dict('split')['data']\n",
    "valid_annotations = annotations[annotations['image'].isin(valid_images)].to_dict('split')['data']\n",
    "for x in train_annotations:\n",
    "    if not x[1].endswith('.'):\n",
    "        x[1] += '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ae2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@padl.transform\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn, proj, embed):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.proj = proj\n",
    "        self.embed = embed\n",
    "        \n",
    "    def forward(self, hidden, input_ids):\n",
    "        return self.proj(self.rnn(self.embed(input_ids), hidden)[0])\n",
    "        \n",
    "        \n",
    "@padl.transform\n",
    "class Greedy(torch.nn.Module):\n",
    "    def __init__(self, rnn, proj, embed, end, max_len=20):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.proj = proj\n",
    "        self.embed = embed\n",
    "        self.end = end\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, hidden):\n",
    "        input_ids = [self.proj(hidden).topk(1)[1].item()]\n",
    "        it = 0\n",
    "        while True:\n",
    "            hidden = self.rnn(self.embed(torch.tensor([input_ids[-1]])[None, :]), hidden)[0]\n",
    "            input_ids.append(self.proj(hidden).squeeze().topk(1)[1].item())\n",
    "            if input_ids[-1] == self.end:\n",
    "                break\n",
    "            if it >= self.max_len:\n",
    "                break\n",
    "            it +=1\n",
    "        return torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f5d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioner = padl.transform(torch.nn.Linear(768, 512))\n",
    "rnn = torch.nn.GRU(64, 512, 1, batch_first=True)\n",
    "embed = torch.nn.Embedding(tg.tokenizer.vocab_size, 64)\n",
    "proj = torch.nn.Linear(512, tg.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22054ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Greedy(rnn, proj, embed, tg.tokenizer.encode('.#')[0])\n",
    "logits = SimpleRNN(rnn, proj, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf64bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.model.config.output_hidden_states = True\n",
    "image_features = (\n",
    "    padl.transform(pl.preprocess)\n",
    "    >> padl.transform(lambda x: x['pixel_values'][0])\n",
    "    >> padl.batch\n",
    "    >> padl.transform(pl.model.vit)\n",
    "    >> padl.transform(lambda x: x.last_hidden_state[:, 0, :])\n",
    ")\n",
    "\n",
    "@padl.transform\n",
    "def myloss(x, y):\n",
    "    targets, lens = y\n",
    "    loss = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        loss += torch.nn.functional.cross_entropy(x[i, :lens[i], :], targets[i, :lens[i]])\n",
    "    return loss.div(x.shape[0])\n",
    "\n",
    "\n",
    "@padl.transform\n",
    "def mypad(x):\n",
    "    len_ = len(x)\n",
    "    x = x[:15]\n",
    "    x = torch.cat([x, torch.zeros(15 - len(x))]).type(torch.long)\n",
    "    return x, len_\n",
    "\n",
    "text_preprocess = (\n",
    "    padl.transform(tg.tokenizer.encode)\n",
    "    >> padl.transform(torch.tensor)\n",
    "    >> mypad\n",
    "    >> padl.batch\n",
    ")\n",
    "\n",
    "training_model = (\n",
    "    (image_features >> conditioner >> padl.same.unsqueeze(0)) / (padl.transform(lambda x: '!' + x) >> text_preprocess >> padl.same[0])\n",
    "    >> logits\n",
    ")\n",
    "\n",
    "inference_model = (\n",
    "    image_features\n",
    "    >> conditioner \n",
    "    >> padl.same.unsqueeze(1)\n",
    "    >> generator\n",
    "    >> padl.transform(tg.tokenizer.decode)\n",
    ")\n",
    "\n",
    "loss = (\n",
    "    training_model + (padl.same[1] >> padl.transform(lambda x: x + '#') >> text_preprocess)\n",
    "    >> myloss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97065526",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "for p in pl.model.vit.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "o = torch.optim.Adam([p for p in loss.pd_parameters() if p.requires_grad], lr=0.001)\n",
    "loss.pd_to('cuda')\n",
    "    \n",
    "for it, l_ in enumerate(loss.train_apply(train_annotations, batch_size=250, num_workers=5)):\n",
    "    o.zero_grad()\n",
    "    l_.backward()\n",
    "    o.step()\n",
    "    print(f'TRAIN iteration: {it}; loss; {l_};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ae8ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "image = valid_annotations[random.randrange(len(valid_annotations))][0]\n",
    "\n",
    "display(PIL.Image.open(image))\n",
    "inference_model.infer_apply(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
