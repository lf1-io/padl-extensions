{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f172b021",
   "metadata": {},
   "source": [
    "PyTorch-Lightning is a package for PyTorch which delivers flexible routines for training models incorporating the latest best practices for deep learning optimization, variable precision, and toggle-on/ off switches allowing easy scaling to multi-GPU and multi-node parallelism. This allows the data-scientist to concentrate on the science, and reduces boilerplate associated with training.  In this post, we discuss how to leverage this power together with PADL, to get an additional shunt into hyper-space.\n",
    "PADL is a functional model builder for PyTorch allowing for full export of model pipelines including preprocessing, forward pass and postprocessing. In addition PADL offers some super handy usability features, such as operator pipeline building via operator overloading, interactive/ notebook friendly design and tools for pipeline inspection and debugging. Follow us on GitHub, and read more about PADL in the official docs and on the developer's blog. The full notebook may be tried on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install padl-extensions[pytorch_lightning]\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf test/lightning_logs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94131097",
   "metadata": {},
   "source": [
    "We can use the full range of PyTorch functionality with PADL. We'll also import `pytorch_lightning` and some connectors from the PADL-extensions package `padl_ext`. PADL also allows you to easily incorporate components from the entire Python ecosystem in your pipeline - for instance, `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c6bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import padl\n",
    "from padl import transform\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from padl_ext.pytorch_lightning.prepare import LightningModule as PadlLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae64878",
   "metadata": {},
   "source": [
    "For simplicity, we'll use MNIST data in this tutorial. You can use any data sets with PADL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de45f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "val_data = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e0237",
   "metadata": {},
   "source": [
    "We'll be using a simple convolution net on greyscale images. We wrap the class definition with the decorator `@transform` which allows the layer to use all of the cool PADL functionality, while also profiting from the usual PyTorch features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33946dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339d9ed",
   "metadata": {},
   "source": [
    "We can now build the hybrid PyTorch/ PADL object into some pipelines, which we'll use to train and test the layer.\n",
    "PADL makes use of operator overloading, which makes it fun and simple to combine PADL transforms and pipelines.\n",
    "`>>` means to compose components (\"transforms\" and \"pipelines\"), `/` means to apply components in parallel. See more [here](https://medium.com/padl-developer-blog/padl-is-the-next-ml-ops-tool-you-should-learn-3e4ba6c66e6e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20863e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"train_model\":\n",
       "\n",
       "   \u001b[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ args                 ▼ args\u001b[0m\n",
       "   \u001b[1m0: \u001b[0m\u001b[32m[\u001b[0mpreprocess: ..\u001b[32m>>\u001b[0m..\u001b[32m]\u001b[0m \u001b[32m/\u001b[0m Identity()       \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mBatchify(dim=0)     \n",
       "   \u001b[32m   │└─────────────────────┐\n",
       "      │                      │\n",
       "      ▼ x                    ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mSimpleNet()          \u001b[32m/\u001b[0m type(torch.int64)\n",
       "   \u001b[32m   │\n",
       "      ▼ (input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mcross_entropy       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = (\n",
    "    padl.transform(lambda x: np.array(x))\n",
    "    >> padl.transform(lambda x: torch.from_numpy(x).type(torch.float))\n",
    "    >> padl.same.reshape(-1, 28, 28)\n",
    ")\n",
    "\n",
    "train_model = (\n",
    "    preprocess / padl.identity\n",
    "    >> padl.batch\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    "    >> padl.transform(F.cross_entropy)\n",
    ")\n",
    "\n",
    "train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5057842",
   "metadata": {},
   "source": [
    "The output of the train-pipeline is a scalar loss (`padl.transform(F.cross_entropy)`). This differs from how you might normally write your training. Usually in PyTorch you would write a dataset, then wrap it in a dataloader, then define a layer and loss. In the training loop you'd fetch batches from the dataloader, push them through the model, and then evaluate the loss on the outputs. In PADL this logic is handled inside the pipeline. That makes it super easy to define very useful objects for talking to the PyTorch ecosystem, as we do here with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507220b",
   "metadata": {},
   "source": [
    "Let's test the pipeline on a single data point. To do that we'll use `.infer_apply`. There's also `.eval_apply` and `.train_apply` which allow batching (see this fully worked out [example](https://devblog.padl.ai/build-a-fully-portable-gan-pipeline-with-padl-and-pytorch-996f584efb17) of pure PADL training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e421f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namedtuple(simplenet=tensor([[ 0.0858,  0.1714, -0.5820, -0.7557,  0.5904,  0.1955,  0.7625, -0.2690,\n",
       "         -0.2968,  0.0700]]), out_1=tensor([5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model[:-1].infer_apply(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb998a",
   "metadata": {},
   "source": [
    "We can define an auxiliary model with weights tied to `train_model` simply by reusing the layer. This model is useful in practice, since it outputs predictions as raw floats. This can then be plugged straight into the server or wherever. We could have also created a JSON output or whatever we liked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f608f684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1mCompose\u001b[0m - \"infer_model\":\n",
       "\n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m0: \u001b[0mlambda x: np.array(x)                          \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m1: \u001b[0mlambda x: torch.from_numpy(x).type(torch.float)\n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m2: \u001b[0mreshape(-1, 28, 28)                            \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m3: \u001b[0mBatchify(dim=0)                                \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m4: \u001b[0mSimpleNet()                                    \n",
       "   \u001b[32m   │\n",
       "      ▼ args\u001b[0m\n",
       "   \u001b[1m5: \u001b[0mUnbatchify(dim=0, cpu=True)                    \n",
       "   \u001b[32m   │\n",
       "      ▼ x\u001b[0m\n",
       "   \u001b[1m6: \u001b[0mlambda x: x.topk(1)[1].item()                  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_model = (\n",
    "    preprocess\n",
    "    >> padl.batch\n",
    "    >> simplenet\n",
    "    >> padl.unbatch\n",
    "    >> padl.transform(lambda x: x.topk(1)[1].item())\n",
    ")\n",
    "infer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebd31b",
   "metadata": {},
   "source": [
    "The PADL extensions package contains a PyTorch Lightning plugin. This is a very lightweight extension of the PyTorch Lightning module. In the usual way with PyTorch Lightning, we can extend functionality by overwriting the default methods. `MyModule` instances are PyTorch Lightning modules, but also PADL objects. This has some very handy advantages when saving the results of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72046679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(PadlLightning):\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d82ce3",
   "metadata": {},
   "source": [
    "The difference of using `PadlLightning` compared to the standard PyTorch Lightning case, is that the amount that needs to be defined is drastically reduced. Due to the structure of the PADL pipelines, methods usually defined manually may be determined automatically. ***That means these methods don't need to be defined***:\n",
    "\n",
    "- `train_dataloader`\n",
    "- `valid_dataloader`\n",
    "- `test_dataloader`\n",
    "- `train_step`\n",
    "- `valid_step`\n",
    "- `test_step`\n",
    "- `on_save_checkpoint`\n",
    "\n",
    "In fact, in the majority of cases, the `PadlLightning` object may be used directly out of the box.\n",
    "\n",
    "If you prefer to *also* save an inference model rather than just the training model, then this may also be passed to the Module. That means that the training model will be used to compute losses, and the inference model's weights will track those weights, and be saved whenever PyTorch Lightning monitoring determines that a good model has been found. All of the standard PyTorch Lightning functionality may be used as with a standard PyTorch Lightning trainer. This comes in very handy, when the way the layer is used in inference is very different from the way it's used in training (think beam search in neural translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3f1f50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "model_checkpoint = ModelCheckpoint(monitor=\"val_loss\", every_n_epochs=1, save_top_k=1)\n",
    "callbacks = [early_stop, model_checkpoint]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    gpus=0,\n",
    "    val_check_interval=10,\n",
    "    max_epochs=10,\n",
    "    default_root_dir='test',\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "padl_lightning_module = MyModule(\n",
    "    train_model,\n",
    "    trainer=trainer,\n",
    "    batch_size=256,\n",
    "    num_workers=0,\n",
    "    inference_model=infer_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21236b0d",
   "metadata": {},
   "source": [
    "Let's fit the module on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "190f9a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | simplenet | SimpleNet | 214 K \n",
      "----------------------------------------\n",
      "214 K     Trainable params\n",
      "0         Non-trainable params\n",
      "214 K     Total params\n",
      "0.857     Total estimated model params size (MB)\n",
      "/Users/dodo/Attraqt/tenen/.venv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory test/lightning_logs/version_7/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a60ae13bfd4eda849425f189e75659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26f9090b5734a1ebfffdaa1a487a51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "padl_lightning_module.fit(train_data=train_data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f74a38",
   "metadata": {},
   "source": [
    "Now let's try a few predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35233d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyUlEQVR4nNWQsQ7BUBiFvxprtNdaq86MHqBvYTR3wwvYBJuY8AJGxpswEa9wxVZh6X9rKI3UZZQ408k993z5c+DPFSnf9hyaPVS16dmyWOYQJFK39SQCAjHvYRRLF6zNcj/rWZvLJOtBkGivwDQSPmxHVPFO3Xr6YZKHJQAW7myV/3QKTtLdGKDhg1e5HSepc9jkWHXaGzFyUUoZPVUHSfULwzv7wPUIMmrj+s319m0JCK3LPqRe9ikVQ2dg42WqxY3PVGpfsl/oDjauVHRBJn48AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164CEC7C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/ElEQVR4nGNgGGSAEc5iYrWVU1CNYLi0sfszuirROX9hYDIXuk616wwMfz4xMDCw8TCIvkPTqfTkSY8/AwMDZ97fzwK4HLH4718PHFJyvu/+/pXAJiPvvfz1379/d3NgSim2P4M49p4ghpz0E7hXTGBiLDAGtyQDA8PFj4dtTTkTzqDrlNm5c64tPwMDw46/T/hweYUh7u9fITQhLlcY6yC6JHvEihtQpud3dEnhv39XITSWMaNJPuKGsBSeYISQ8N/X6gwMDAwMFW///i1A1cjAff7vp+sFBX1vfv79uwxNjoFB+DwseGZLYvpOePLfv3///p3TKIYpR20AAN9gdO4LuPxVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C1C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+ElEQVR4nM3PPUtCYRTA8UO3IHyQuAZXERpaWgwMmpr6GKFDfoFwjKjljjn3BRqaxC3pBaSaveggQUMYQhGBBE29wP/0NDV4PWvQmQ7nx3kT+e+xsNNW/7JiUVS/B5SqiIjMTljxdM33XyW3npnu2xiRbGWlpuPVadwHJ3L4pp/PHZeyuWuNRcJb9ao3aSxDVNx7gq/+QZieWoarB3g83zT+qAA6jpesH10CtHMWifOq2pgozfwm4dm3995sk8UOH/UuQxNPYFsuGFhWeudIojsahgVNTVywq5o3sADHURNa8zZe9pRu1lpZANDWsnlrUBpCbM38i/gB3C9yvyg4rqgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nM3OLQoCYRCH8UHdIogIYjCIrgcw2MSwGC0Wi80DGDVbBD+SzStYBUGLJk+wwbBgEEwewPQMa7G4zFvFaTM//jMj8gfljfYaOKy8AE6fJpPAS0VEsmbOXysQdk1coxCWTJupxlHffqb5gKhmW+EBt6pt0gGGDpMVbFMOC2LVSdqBYwUmX6PEnrnjoVABIt9MFkXug1e9ZUdjPco01p6JyqXxVA4mLgGUjYneDtBrzj6aX8K5bdvP6g3+BlTiLpX9TAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9CB20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nGNgGGSAEc7iqfLVvnCIYentD1iURf79+/fvv79/p2KREzv198Csj//+/t2DRXLh35csDJzcXGlWWCSX/W3G6TCh1391cbpW/BlD+zcGhkMn/mDRqfX379+/f//+2+HAhCkZ+vfv379Hvvz7+zcWq2SyIqds3obXb2sxJLnyPSGMo/+e6eF0tvzKv0txSjJo/X0nBGViuu0tAz8LNkmtDAYGBgkcJu7568egvP7vXzEonwVVet0bTh6Gv/+x6cz58f/v379/J2I3t/vf379XZ/Bhl7Tvv7NQBIeDqAcABO9Z1klHDfUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C8E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nGNgGDaAEYuYgzTfy01/MJUaF0gzMBR+//f38GlfVCm20NV//3ozMDAot6298PkvquSiv38/FHIyMDAwMGgk7r2IItfx/3SiBgMDAwOD0PJjp5ZpoEg++O7KwMDAwMDSdDOWD901Dx4wMDAwMBgcjhPG9MHMBwwMDNJ9U/Wx+I6B8yWftPc8EzZscgwM/99ss8Quw8Ag9O+vEaoIE5ylv/Epw2fs2rgr3utf++uBVY5z+5lwln9t3KiiLBCqTsbjKQOjwFesOj8eZGCw+rcBu5Wbf7kzH38mgiYKjWxJ5T+XDZ/fxa6T2gAA9ztFxjDrBEMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nGNgGG4g7O8BJ2EIU9i/QQpVcsGfv38erMyxtLRc+fDPny9BKJJH//z98weBj0BEmSDUHKyWsUCor4xMPyZJ2f9nmPNi66RAxkMokv///ztbDmHyqv5nOIdigPDzvx90IEzLP3/+hqDY+fYEA08BhJnCwPDlNoqxDC3G/09ABMQYGTZdxOo+BgazP3/+qOGQY9j05881XHIMf//86cYlV/fv7zleHHLCz//+icWlMfTP3wsIjUwY8hM+49DIfubve5z+sPzz5wQSF8PYQFwaGRSe/MEphw4Ax7JZRaPTRa4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAyUlEQVR4nM2NPwtBYRjFTyxX/kxksymlyyCbzcB+B8lGyWTxRZgln0ApuZtsFpZ7YzcqA8VF6bwsEvW8m8FZntP5dc4D/L/aMzE2QgCMFfMSHE6CQIfHsMCqHstIbNiXinseY+jxkhFY9qoqiDmqKxUd7uIYkIV34nu7iIlNpJkDSkKxpkhSkWOhuX1ddyT9tKdzpR6WhADAv+Ta0EHTY0vHsOAlqmOpE+tfge/DN4J3V7u6p61lyTOL38nHbDrAgxZauHna2V/qCfC9RTdpPK0YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C4F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4nGNgoApQVcYpxZz89v18JhySAk/+/fs3G5fWBf/+/ft7HIckr9XGt/9+F+O0V/zyv9PsOGW1vvwThnPQXffmAxIHXVJaCo8kCmCBszjZGCw0GOyxKVKbcu8fFPiiy5348g8Ofh3gR5X8/+/fv79fvkGlb1ujSP7792+hH0Ppv3//7m398e/fhWWeSJbP+PevWD3n479/T+QZ1I7++/fv369ncEl7qIF3IxgYGJh3/vv3798/uFduXNRnYGBgYNi6goGB4W+io6oVsqWS8+78/PdvNyeKSxgRTFNOhif3sIXBEAEAtV92CrZqFFAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C1C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA30lEQVR4nN2OP0tCcRSG3+5SQnQnXXQ2JMVJoiGssYbmBhdRXAVHlwYFV7+ELU1BW5u4ud0xxDWSCg2iP/D8clKuPzlfoHc65zznfc+R/o121tV++2ugcrGS6PS//a3wAcABHK9mwarIXaz3jrbyWzB/p/c8h9+kD2+hlr6W6kDKY3sjPlOSVIfpgXczPGE4k6SSNPzwnInauSTp9C32ra97eM0aLIygYRmbzj1ZTA7yFqs47gKD7U7+qFrGK1wUxvqNkEPpZWHA4FIaW6kF+Dmz4A08bgzisRmpaxm3tQRz8VE/PLad2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9C970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    image, label = val_data[random.randrange(len(val_data))]\n",
    "    display(image)\n",
    "    print(padl_lightning_module.inference_model.infer_apply(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6fd15",
   "metadata": {},
   "source": [
    "Now we can make use of a major practical advantage of PADL: ***saving and loading are completely self contained and take care of all aspects of the pipeline***.\n",
    "That means that the following cell works in a completely new session! You can restart the kernel, do this in a new session/ server etc..\n",
    "That makes the results which you obtained with the PyTorch lightning trainer super portable and reusable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd4df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "m = padl.load(padl_lightning_module.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b58fc1",
   "metadata": {},
   "source": [
    "Let's verify the predictions of the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93279f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuUlEQVR4nGNgGAUMjCg83gLNjwzfbxgyKJo91kdT6XH039fP//79+/fv/992NJ0pfTxP8o56/U3eyMBw+CyKNr4Zf740yOCw3enfv+3iuJzGde/fv1e7J6hjl5U8dv/fv3+vLDAkeCCaY1sP/jsoDRdlYmBgYGAQncvGwMDA8PfQMdE/oixoGvfeVuAW4FY68e/fvx1C6KYG//t369mdJ/9+3crgRhKGBIKoDgNDACvD+w2ncfllqAMAQMZDZBXDSMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E422E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nM2PL4gCURjER9FgULlgEbvBdMGsSfCSICIGzQa7YLm22SJG4TjbtWuLqEWbqMVgEzUJsiLsrjCPNaz/wPei4JThze+bj/cBL1deE8L5DspQ2rBJCg4/JFBnrxmNdiyWJPD/KwgABkeSasw1jUxeI+8NblzbPsx78aSjJfsvAGDAP3XzEydlMW0yrmSO6KhYxhTTiCTPVRqT2oFzGcuuSJLcFQP30ONa4cd/eTvzWTjWXYjhfWZCklyOx8JdQFsH4AMAVBMALO13jZxTD7U9VZxat6Igj/2s4gTD3JdV5wEpNXonnQHwOl88qq0MFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E42250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4nGNgoAb4V4smwIRgBvzXxqlP4f7fEpw6FWQZTuOU1MHnnJV/D+GUM3nxxx2nZPufP7hNffHnEU45959/0mFs6awrl7OQJff/3Qdjyq/7++zzGz4GhFf+//8PZTls87tjukLADUnnvj97IQyWw3+uyDJof9rCgBK2UFdb/s5+zMDHwolNsohhxkEGBhk2BiRJRkZGBgYGBgY7JsZNDAwcJYyHkTQEf3s7iYGBgSHr7wM+BpWiPydFkY2r/frnhoEIQ/HfS8FHP/6aIYxqWfDev3++rnz/98+fNxPdGDCAzeQTf/78/TPbCVNqKAAA0qNTUfOfMIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E42CD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABGUlEQVR4nGNgoBNQuPP/fzaEyYgikcPAUCDBybAu+heGHrsn//7+/fv3700uTPPE3/6FSD6Sg4owIST/fmFgeLK3m4Hh9zeoCAtC8o2bNONjmzwGhh9vMCUZPjIx2EzmYPgxA4sf0q9B7CzGIidx7S9E0hMuhHAQ3BmztWBCSIEQZsDA0Ki9S4jhhhYDDhD+9+9VTGOh4BcDg0oIilfklogwvJ5wVZvh3kUGBgYWThTlJ/7+/fv378O//56fvPn3799YZLmA738hABK29yWRjeVmYrjb8I2BgSEomoGB4UffcxRj+/7+3bnQR2nhub9//37LQXOixu1ff//+/fv3/9+/x/0QwrBAiDfJYmBgOLSTYfZbXCEwCAAAfyKAwHC3EBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E425E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA50lEQVR4nM3Qv6uBYRQH8HOeQa/Se6VMynJL6Q78BS7DHSn/gDIrg27d0XLdRQaz/AGuxWyw3xUlSlHSuyklyffI4Ee8nme7g+94Pn3PqUP0Twn0J2Ej1gTTdz2pr51gn9ZjHBD8GJaOABn7tGQ1RWQY0vcyAMYG8/4ByBkOfgtQtfUWWgh6BqOWHCRlML8DmdlERJnkA7YB+aSXYt3Zb52CCx1gHozOIQCwqnjcOKARTig4XVdnZGarFGFWzMyKE+7muQRAELvD7OYGf2PqDqm8vmLjMuOrvn685S3izrLb3Rme8dw5AuqlgIHhztazAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E42190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABFklEQVR4nGNgGGSAEZnDJBCszJDDxfi/p+oPukLRxX9hoJaBgYGBgQUhN8NGk+HLdYapXxkUunXR9Em+/LNtsREDAwMDQ+DfF1Kokjv/ToMpe/v3rzKq5Pm/b/UZGBgYGMw3/8WQNH7/dyUDAwMD256/jxsbhdAsnfLnswMDR83nv4/Usfj5/t93vgv//p2BTY5B8/Hfv3//TmbCJsfA0Pf377MWNgQfWRmzFgMD9+lfWPWlrv677c7fedikVNf8/WrJtBFFEmasaF7gM7fj/7Aayb7771sJBgaZx387MCVV/r52YWBgn/73pRySKDzKHglwuXklMyx4hKlT7O7fv/f//P07mwObpTEr/v79+6mEFauLqAwA0Rxs/lV2I9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E425E0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nGNgGMJAc+a7my445Gb9vbnh8jctbFICh/7msjLw3ozCJrn7nx8zAwPD4WlY5Or/5jAzMDCwX2rAlAv70sHMwMDA4PIvDlPy4RleBgYGBu5b/xwx5Hz+QMQEn97iwJA8+k+KgYGBgX3fL1tMU+//02BgYDA6928HFre6/X7eXnf8+79/bggxRjjLIcLo5iPGytuGX7FoZWBgYGCo+7cciceEJsv4CLekyv/9uAxlYL7+lBenZPa/OchcVGPFGfeb4NS57/9HG5ySDVesccqRBAD3Lj/C98Yp5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164D9CB80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABIElEQVR4nM2QsUtCURTGfy+EQHgguPQeLxqK5lpC/weXWlyDiGiocGgJgyBobSsoW1uLmgoJAoUHluBQSEKgZUFDS0Jk53Jb8ul94hh0psP3u9893znw1zV8o05NZajLdqf07SDjulwmQlLXOULeHwjt7/zAOK4q9GmRTpMNpKRH5cF8VVfzAOw3PkTeNw0WbTaAyExT6beTun4e64WrsgfulsjTjgdnkuidOU0Nskv6KnMH1MxALjCZ5nCtDUC53Atty2Ildrzc2bkN3SNorXG08/vNwouRtigZ4q+fG3EAv2Vc2X2UDDglubZJ+V/mmlyo8yiQGh89UK0Qw6uowlwyuV0UdT9LuJyqiCgRycUDzQq6WHpiMac5qvb5/k/9AF+bZJFBxVz7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E42280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3ElEQVR4nGNgGEqAEUKlzmRgYGC4v5nhyTqGV1/QJOVnvGEw5ZFkYPzPwLAiGk2SgYGBQZhDkIHBfBbDSSsctvA2//0dikPO8Mbfv23YpQSqv/975sWMVc764N+fywSQRZgQTHsbhpvbZXWZGLCByL/Pvvz/+3d/iSAWSSZFcdn+NX///t3LhcO9DAzZ278fxinJwLDqE6aDYEBMAY/Gor/rccpF3L+EzbkMDAw8kVueXOLBKqW96s3fL+ux6OMKmT37yd+nLViN7Pz798OGAFnsti27HiWN05EkAwCiL0SByQDW2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E42130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4nGNgGLSAQ7Rj9r9////97DDAkKs/8hcGItDlDN9BZZ79+bsCJsgCpcX5GRhuTL/KwBCRzMCKrpNXX19flIGBYd3Pv5+tcLiq/Offv+uwyjBKzfj99+8nM6xyqX///v17khmrRjaIc+cI4ZH8e9gQiyRT7YZDf//+/fv3UxRWk7k0NTWz/v7dg1WSgYGBgWnu3x8GUCaGJOtRBtZi7Pokg0/9/XtdDEVMWYuBgYHVwMDg7t+/f78koWroPhnuVLPv79+/f//+2oMmx7AaHpk3TDCscoNK3TcTQRKFxucHBgaGfz1v5/7+gsOD1AQAOYB+YF2YyW8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x164E425B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    image, label = val_data[random.randrange(len(val_data))]\n",
    "    display(image)\n",
    "    print(m.inference_model.infer_apply(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98873e04",
   "metadata": {},
   "source": [
    "Continuing training is a simple as pie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b108136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at test/lightning_logs/version_7/checkpoints/epoch=0-step=178.ckpt\n",
      "/Users/dodo/Attraqt/tenen/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:250: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint file at test/lightning_logs/version_7/checkpoints/epoch=0-step=178.ckpt\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | simplenet | SimpleNet | 214 K \n",
      "----------------------------------------\n",
      "214 K     Trainable params\n",
      "0         Non-trainable params\n",
      "214 K     Total params\n",
      "0.857     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e6ca0c230c4312bb6ffccf5b89e6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.fit(train_data=train_data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e46111",
   "metadata": {},
   "source": [
    "PADL plus PyTorch Lightning mean in future that your design patterns can take a very satisfying stereotyped form:\n",
    "\n",
    "1. Define preprocessing, forward pass, postprocessing, loss into one or two pipelines.\n",
    "1. Pass the pipeline to the `PadlLightning` trainer.\n",
    "1. Save the trainer (which saves the contained pipelines).\n",
    "1. Reload the trainer in one line of code.\n",
    "1. Continue the training on the updated/ latest data.\n",
    "1. Resave the trainer.\n",
    "1. Rinse and repeat previous steps ad infinitum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca4d22",
   "metadata": {},
   "source": [
    "And that's all there is to it. Nice pipeline definitions in PADL trained at warp speed with PyTorch Lightning.\n",
    "\n",
    "**Happy PADL-ling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
