{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5492e1e4",
   "metadata": {},
   "source": [
    "## Install `PADL-Extensions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install padl-extensions[pytorch_lightning]\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74765ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These might be useful if there are errors regarding ipywidgets while downloading torchvision.datasets\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac4c58",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import padl\n",
    "from padl import transform\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd944ba",
   "metadata": {},
   "source": [
    "## Using PADL with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de455b",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "MNIST dataset available through torchvision is used in this notebook. The dataset can be separately downloaded from MNIST website or can be loaded as given below. \n",
    "\n",
    "More details on torchvision's MNIST dataset can be found here: https://pytorch.org/vision/stable/datasets.html#mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de45f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "mnist_test_dataset = torchvision.datasets.MNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a4df6",
   "metadata": {},
   "source": [
    "## 1. Model Definition\n",
    "\n",
    "We will build a simple `Unet` to classify `MNIST` handwritings. In the cell below, a simple `torch.nn.Module` is defined with the decorator `@transform`. This is enough to wrap the pytorch model into a `padl.Transform` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.models.resnet \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "@transform\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv 1\n",
    "        # size : input: 28x28x1 -> output : 26 x 26 x 32\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv 2\n",
    "        # size : input: 26x26x32 -> output : 24 x 24 x 32\n",
    "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv 3\n",
    "        # size : input: 24x24x32 -> output : 12 x 12 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=2, stride = 2)\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Conv 4\n",
    "        # size : input : 12 x 12 x 32 -> output : 8 x 8 x 64\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv 5\n",
    "        # size : input: 8x8x64 -> output : 4 x 4 x 64 -> Linearize = 1024\n",
    "        self.conv5 = torch.nn.Conv2d(64, 64, kernel_size=2, stride = 2)\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(64)\n",
    "\n",
    "        # dropout layer \n",
    "        self.conv5_drop = torch.nn.Dropout2d()\n",
    "\n",
    "        # FC 1 \n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "\n",
    "        # FC 2\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm4(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm5(F.relu(self.conv5(x)))\n",
    "        x = self.conv5_drop(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def convert_to_tensor(img):\n",
    "    arr = np.asarray(img)\n",
    "    return torch.tensor(arr).type(torch.FloatTensor)\n",
    "\n",
    "preprocess = (\n",
    "    convert_to_tensor / convert_to_tensor\n",
    "    >> padl.same.reshape(-1, 28, 28) / padl.Identity()\n",
    ")\n",
    "\n",
    "simplenet = SimpleNet()\n",
    "loss_func = transform(F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9505378",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device to be used: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ed251",
   "metadata": {},
   "source": [
    "Let's define our training model which must include the loss as the last step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d949879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = (\n",
    "    preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> simplenet / padl.same.type(torch.long)\n",
    "    >> transform(F.nll_loss)\n",
    ")\n",
    "\n",
    "train_model.pd_to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2181e6",
   "metadata": {},
   "source": [
    "For inference let's define a separate pipeline that will return the predicted number of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_preprocess =(\n",
    "    padl.same[0]\n",
    "    >> convert_to_tensor\n",
    ")\n",
    "infer_model = (\n",
    "    infer_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.same.unsqueeze(1) \n",
    "    >> simplenet\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")\n",
    "infer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404e855",
   "metadata": {},
   "source": [
    "## 2. Converting a PADL model into a Lightning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de0b17",
   "metadata": {},
   "source": [
    "The class `PadlLightning` is already a `LightningModule` so inherting from it allows for all the regular customizations available in Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl_ext.pytorch_lightning import PadlLightning, padl_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93212691",
   "metadata": {},
   "outputs": [],
   "source": [
    "PadlLightning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df265d6",
   "metadata": {},
   "source": [
    "Any defaults in `PadlLightning` can be easily overwritten. For example, let's overwrite the default optimizer in `PadlLightning` by overwriting the function `configure_optimizers` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72046679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(PadlLightning):\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 0\n",
    "\n",
    "padl_lightning_module = MyModule(\n",
    "    train_model,  # train_model with the loss function\n",
    "    train_data=mnist_train_dataset,  # list of training data points\n",
    "    val_data=mnist_test_dataset,  # list of validation data points\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "# padl_lightning_module is a LightningModule !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c73710",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "First we set up the pytorch lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "nepoch = 1\n",
    "ngpus = 1 if device == 'cuda' else 0\n",
    "\n",
    "# Define callbacks to be given to the trainer\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "model_checkpoint = ModelCheckpoint(monitor=\"val_loss\", every_n_epochs=1, save_top_k=1)\n",
    "callbacks = [early_stop, model_checkpoint]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    gpus=ngpus,\n",
    "    val_check_interval=10,\n",
    "    max_epochs=nepoch,\n",
    "    default_root_dir='test',\n",
    "    log_every_n_steps=log_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4a7d4",
   "metadata": {},
   "source": [
    "Now let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl_ext.pytorch_lightning import padl_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(padl_lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb8939",
   "metadata": {},
   "source": [
    "## 3. Infer a few images from the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119174e",
   "metadata": {},
   "source": [
    "First we define a few transforms to plot the images from the MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2abb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@transform\n",
    "def plot_image(img_tensor):\n",
    "    fig= plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_tensor, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return fig\n",
    "\n",
    "@transform\n",
    "def img_to_array(img):\n",
    "    return np.asarray(img)\n",
    "\n",
    "convert_plot = (\n",
    "    img_to_array\n",
    "    >> plot_image\n",
    ")\n",
    "\n",
    "plot_datapoint = (convert_plot - 'image')/ (padl.Identity() - 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c208d",
   "metadata": {},
   "source": [
    "Now we plot the MNIST image and the model prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd69eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    data_point = mnist_test_dataset[np.random.randint(len(mnist_test_dataset))]\n",
    "    output = plot_datapoint(data_point)\n",
    "    plt.show()\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48c6eb",
   "metadata": {},
   "source": [
    "## 4. Inspect the pytorch lightning checkpoint\n",
    "We can inspect the checkpoint file saved by the pytorch lightning trainer using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = trainer.checkpoint_callback.best_model_path\n",
    "print(ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_file, map_location=torch.device(\"cpu\"))\n",
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9655be",
   "metadata": {},
   "source": [
    "As we can see pytorch lightning saves quite a lot of information about the state of our model, optimizer, and trainer. Additionally, we have added the keyword `padl_models` that show the locations of all saved PADL models that resulted from this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['padl_models']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824086b",
   "metadata": {},
   "source": [
    "## 5. Restart training from the Pytorch Lightning Checkpoint\n",
    "To intialize `MyModule` from a pytorch lightning checkpoint file we will need to use the `MyModule.load_from_checkpoint` function. We will also need to provide some additional arguments such as `padl_model`, `train_data`, and `val_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11329348",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_module = MyModule.load_from_checkpoint(\n",
    "    ckpt_file,\n",
    "    padl_model=ckpt['padl_models'][-1],\n",
    "    train_data=mnist_train_dataset,\n",
    "    val_data=mnist_test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e232f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(loaded_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b16d45",
   "metadata": {},
   "source": [
    "## 6. Export the trained `torch.nn.Module` layer into a separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98bfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padl import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(ckpt['padl_models'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075cb9c",
   "metadata": {},
   "source": [
    "From inspecting the loaded model we can see that we can access the trained layer at position 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed022b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4601937",
   "metadata": {},
   "source": [
    "Acessing the trained layer can be either be done by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84365ac9",
   "metadata": {},
   "source": [
    "or by providing the name of the `padl.transform` which is `\"simplenet\"` for our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696de3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model['simplenet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df338a",
   "metadata": {},
   "source": [
    "Create an inference model using the `simplenet` layer from `loaded_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_preprocess =(\n",
    "    padl.same[0]\n",
    "    >> convert_to_tensor\n",
    ")\n",
    "trained_net = loaded_model['simplenet']\n",
    "infer_model = (\n",
    "    infer_preprocess\n",
    "    >> padl.Batchify()\n",
    "    >> padl.same.unsqueeze(1) \n",
    "    >> trained_net\n",
    "    >> padl.transform(lambda x: x.max(1).indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    data_point = mnist_test_dataset[np.random.randint(len(mnist_test_dataset))]\n",
    "    output = plot_datapoint(data_point)\n",
    "    plt.show()\n",
    "    print(f'Prediction: {infer_model.infer_apply(data_point).item()}')\n",
    "    print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
